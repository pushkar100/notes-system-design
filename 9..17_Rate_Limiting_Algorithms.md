# Rate Limiting Algorithms

## Table of Contents

- [Rate Limiting Algorithms](#rate-limiting-algorithms)
  - [Token Bucket](#token-bucket)
  - [Leaky Bucket](#leaky-bucket)
  - [Fixed Window Counter](#fixed-window-counter)
  - [Sliding Window Log](#sliding-window-log)
  - [Sliding Window Counter](#sliding-window-counter)
    - [How is sliding window counter better than sliding window log?](#how-is-sliding-window-counter-better-than-sliding-window-log)
  - [Comparison of algorithms](#comparison-of-algorithms)

## Token Bucket

Concept: Imagine a **bucket** that gets **refilled with tokens at a constant rate** (e.g., `5 tokens/sec`). Every time a *user makes a request, they must take a token*. If the bucket is empty, the request is denied!

Key Feature: **Allows bursts of traffic**. If the bucket is full, a user can make many requests instantly until it empties...

Analogy:
- **The Bucket**: Represents the maximum burst capacity your system allows
- **The Tokens**: Represent the permission to perform a request
- **The Refill**: Tokens are added to the bucket at a fixed rate (e.g., 10 tokens per second)
- **The Consumption**: Every time a user makes a request, they must "pay" a token

The Golden Rule: 
- If the bucket has tokens, the request is allowed
- If the bucket is empty, the request is denied (throttled)

Why is this special?
Unlike the *Leaky Bucket* (which forces a constant output rate), the Token Bucket allows for bursts

Key Legend:
- `\___/` : The Bucket (Fixed size buffer/queue)
- `*` : A Request (Water droplet)
- `v` : Input flow
- `|` : The "Leak" (Constant output rate)
- `X` : Dropped/Overflowed request

***The setup:***
```
   [ TOKEN SOURCE ]
    (e.g., 10 tokens/sec)
          |
          | Refill Rate (Constant)
          v *
    +-----------+ <---- Max Capacity (Burst Limit)
    |  * * *    |
    | * * * *   | <- Tokens waiting
    \____*_*____/
         / \
        /   \  (Take 1 token)
       /     \
[In Req] --> [Out Req] (Allowed)
```
***Burst Traffic (Allowed):***
```
PHASE A: BEFORE BURST (Idle)
Bucket fills up to capacity.

    [Source]
       | v *
    +-----------+
    | * * * * * | (FULL BUCKET)
    | * * * * * |
    \___________/


PHASE B: THE BURST OCCURS
Many requests arrive at once.

[Req][Req][Req][Req][Req] ---> ARRIVING RAPIDLY


PHASE C: PROCESSING BURST
Requests take existing tokens rapidly. Refill can't keep up momentarily.

    [Source]
       | v * (slow refill)
    +-----------+
    |           | (BUCKET DRAINS RAPIDLY)
    | *         |
    \____*______/
     / / / / /
    v v v v v  (All Allowed quickly)
[Out][Out][Out][Out][Out]
```
***Rate Limited (Bucket Empty):***
```
   [ TOKEN SOURCE ]
    (Refilling slowly...)
          |
          | v * (drip...)
          |
    +-----------+
    |           |
    |  (EMPTY)  | <--- No tokens available
    \___________/
         ^
         | (Try to take token...)
         |
[In Req] ----> X  (DROPPED / REJECTED)
(Arrived too fast)
```

## Leaky Bucket

Concept: **Requests enter a queue (bucket)**. The queue is **processed (leaks) at a constant, fixed rate**. If the queue is full, new requests are discarded

Key Feature: **Smooths out traffic**. It **forces a constant output rate regardless of how fast requests come in**

***Steady State (Normal Traffic):***
```
        [ Irregular Input Flow ]
            (Slow arrival)
               |       |
               v       v *
    +-----------------------------+ <--- Max Queue Capacity
    |                             |
    |                             |
    |            * (Bucket low)   |
    \____________*________________/
                 |
    (CONSTANT    | * <-- Output is smooth and fixed
     LEAK RATE)  |
                 v
           [ Processed Req ]
```
***Buffering a burst:***
```
     [ SUDDEN BURST OF INPUT! ]
          | | | | | | | | |
          v v v v v v v v v
    +---------------------------+
    |       * * * * * * *       | (Bucket fills rapidly to
    |     * * * * * * * * *     |  absorb the surge)
    \___________*_______________/
                |
    (CONSTANT   | * <-- Output stays the same speed!
     LEAK RATE) |         (No bursts allowed out)
                |
                v
          [ Processed Req ]
```
***Buffering a Burst (Traffic Smoothing):***
```
     [ SUDDEN BURST OF INPUT! ]
          | | | | | | | | |
          v v v v v v v v v
    +---------------------------+
    |       * * * * * * * | (Bucket fills rapidly to
    |     * * * * * * * * * |  absorb the surge)
    \___________*_______________/
                |
    (CONSTANT   | * <-- Output stays the same speed!
     LEAK RATE) |         (No bursts allowed out)
                |
                v
          [ Processed Req ]
```

## Fixed Window Counter

Concept: **Time is divided into fixed windows** (e.g., 1 minute). **A counter tracks requests for the current window**. **If the count exceeds the limit, requests are dropped until the next window starts**

Key Feature: **Simple** and **memory efficient**

***Flaw***: **The Edge Case**. If you allow 100 req/min, a user can send 100 requests at 10:00:59 and another 100 at 10:01:01. In that 2-second span, they sent 200 requests, effectively doubling the limit

Key Legend:
- `|---|` : A fixed time window (e.g., 1 minute)
- `*` : A Request
- `[C=N]` : Counter Value (N)
- `X` : Dropped Request

***Normal Operation (Under Limit)***:
```
Scenario (`Limit = 3`)

Window 1: 2 requests arrive (OK).

Window 2: Counter resets, 1 request arrives (OK).

Start of Window 1            Start of Window 2
      (Counter starts at 0)        (Counter RESETS to 0)
             |                            |
TIMELINE     |----------------------------|----------------------|
             |                            |
Requests     |      *          *          |          *
             |      |          |          |          |
Action       |     (+1)       (+1)        |         (+1)
             |      |          |          |          |
Counter      |    [C=1]      [C=2]        |        [C=1]
             |                            |
Result       |     (OK)       (OK)        |         (OK)
```

***Rate Limiting (Limit Reached)***:
```
Limit = 3 requests per window

Start of Window
             |
TIMELINE     |-----------------------------------------|
             |
Requests     |  *    *    *          *       *
             |  |    |    |          |       |
Counter      | [1]  [2]  [3] (MAX!) [3]     [3]
             |            |          |       |
Result       | (OK) (OK) (OK)       (X)     (X)
             |                    Dropped  Dropped
```

***The "Boundary" Flaw (2x Traffic)***: The main weakness of this algorithm. If a burst happens at the ***end*** of one window and the ***start*** of the next, the system ***allows twice the rate for a short period***

```
Scenario: Limit = 3 requests per window ("1 minute" windows chosen)
Reality: 6 requests processed in just a few seconds

                   Window A               Window B
            (10:00:00 - 10:01:00)  (10:01:00 - 10:02:00)
Limit: 3              |                      |
                      v                      v
TIMELINE  |----------------------||----------------------|
                                 ||
Requests              *  *  *    ||    *  *  *
                      |  |  |    ||    |  |  |
Counter              [1][2][3]   ||   [1][2][3] (Reset!)
                      ^  ^  ^    ||    ^  ^  ^
Result               (OK)(OK)(OK)||   (OK)(OK)(OK)
                                 ||
                     /           ||           \
                [ 3 Reqs ]       ||       [ 3 Reqs ]
                    \            ||           /
                     \_______________________/
                                 |
                  Total: 6 Requests in ~2 seconds!
                  (Violates the intended rate limit)
```

## Sliding Window Log

Concept: You ***keep a log of timestamps for every request***. To decide if a request is allowed, you ***look at the logs and count how many occurred in the last window*** (e.g., last 60 seconds). You ***remove timestamps older than the window***.

Key Feature: **Extremely accurate**; **solves the Fixed Window edge case**

***Flaw: Expensive***. *Storing a timestamp for every request consumes huge amounts of memory at scale*

Key Legend:
- `[=============]` : The Sliding Window (e.g., 60 seconds)
- `@` : A stored timestamp (Log entry)
- `_` : Empty space
- `X` : Expired timestamp (Pruned)
- `*` : New Request

***The Setup (Memory Structure)***:
```
Scenario: Limit = 3 requests per minute

     (Oldest)                        (Newest)
LOGS:    @             @                 @
      10:00:05      10:00:45          10:00:55
```

***Pruning & Allowing (Making Room)***:
- A new request arrives at `10:01:10`
	- Step A: Define the new window `[10:00:10 to 10:01:10]`
	- Step B: Delete (Prune) any logs older than the start of the window
	- Step C: Count remaining. If `< Limit`, add new log
```
NEW REQUEST AT 10:01:10 (*)
Window Size: 60s

      [ PREVIOUS WINDOW ]
      |-----------------------------|
      @           @           @
   10:00:05    10:00:45    10:00:55


      [ NEW WINDOW (SLIDES RIGHT) ] ------>
            |-----------------------------|
      X     |     @           @           *
   (Pruned) |  (Kept)      (Kept)       (New)
            |                             |
   10:00:05 |  10:00:45    10:00:55    10:01:10

LOG COUNT: 3
LIMIT: 3
RESULT: (OK) - Request Allowed & Stored (âœ”)
```

***Rejection (Window Full)***:
- A new request arrives immediately after at `10:01:15`
	- Step A: Slide window to `[10:00:15 to 10:01:15]`
	- Step B: Prune older logs. (None are old enough to drop!)
	- Step C: Count remaining
```
NEW REQUEST AT 10:01:15 (*)

            [ NEW WINDOW RANGE ]
            (10:00:15  -  10:01:15)
            |-----------------------------|
      _     |     @           @           @       *
            |  (Kept)      (Kept)      (Kept)   (Try)
            |                                     |
   10:00:05 |  10:00:45    10:00:55    10:01:10  10:01:15

LOG COUNT: 3 (Existing)
LIMIT: 3
RESULT: REJECTED! (Count 3 >= Limit 3)
        * No space freed up yet.
        * New timestamp is NOT stored.
```

## Sliding Window Counter

Concept: A ***hybrid of Fixed Window and Sliding Log***. You ***don't store individual timestamps***. Instead, you ***use a weighted average of the previous window's count and the current window's count to estimate the rate***

Formula: 
```
Requests = (Requests in Current Window) + (Requests in Previous Window * Overlap Percentage)
```

Key Feature: **Accurate** and **memory efficient**

***The Setup (Visualizing the Overlap)*** (Example): 
- We are currently 30% of the way into the "Current Window"
- This means our "Sliding Window" (the last 1 minute) covers the current 30% and the remaining 70% of the Previous Window
- Scenario: `Window Size = 1 Min`
- `Current Time = 10:01:18` (30% through)
```
      [  PREVIOUS WINDOW   ]  [   CURRENT WINDOW    ]
      (10:00:00 - 10:01:00)   (10:01:00 - 10:02:00)
      Count: 10 requests      Count: 2 requests
      |=====================| |======================|
                  ^                    ^
                  |                    | Current Time (30%)
                  |                    |
      <-----------+--------------------+
      [      SLIDING WINDOW (1 min)    ]
      [ 70% Overlap from Prev ] [ 30%  ]
```
***Calculation: Request Allowed***
- Scenario: Limit = 10 Reqs/Min.
- Previous Window Count: 5
- Current Window Count: 2
- Time: 40% into the current window (means 60% overlap with previous)
```
      [ Prev Count = 5 ]      [ Curr Count = 2 ]
      |================|      |======*=========|
                                     ^
                                     | (40% Elapsed)
      <------------------------------+
      [   60% Weight   ]

CALCULATION:
1. Weight of Previous:   5 requests * 0.60 (Overlap) = 3.0
2. Add Current:          + 2 requests                = 5.0
-----------------------------------------------------------
TOTAL RATE:              5.0 Requests

RESULT: 5.0 < Limit (10) -> (OK) ALLOWED
```
***Calculation: Request Rejected***
- Scenario: Limit = 10 Reqs/Min
- Previous Window Count: 12 (High traffic previously)
- Current Window Count: 1 (Just started)
- Time: 10% into the current window (means 90% overlap with previous)
```
      [ Prev Count = 12 ]     [ Curr Count = 1 ]
      |=================|     |==*=============|
                                 ^
                                 | (10% Elapsed)
      <--------------------------+
      [     90% Weight    ]

CALCULATION:
1. Weight of Previous:   12 requests * 0.90 (Overlap) = 10.8
2. Add Current:          + 1 request                  = 11.8
-------------------------------------------------------------
TOTAL RATE:              11.8 Requests

RESULT: 11.8 > Limit (10) -> (X) REJECTED
```

### How is sliding window counter better than sliding window log?

**The Core Difference**:
- The Sliding Log algorithm is accurate but expensive because it stores data per request
- The ***Sliding Window Counter is an approximation that stores data per window***

**Memory usage**:
- **Sliding Log (Memory Hog)**: If you allow 1 million requests per hour, you must store 1 million distinct timestamps in a sorted set (e.g., Redis ZSet). Memory usage grows with traffic (**`O(N)`**)
```
User_123_Log: [
  "10:00:01", "10:00:02", "10:00:05", ... (999,997 more items) ...
] 
--> HUGE MEMORY footprint (Megabytes per user)
```
- **Sliding Window Counter (Memory Efficient)**: You **only store two integers no matter how much traffic you get**: the ***count for the previous window*** and the ***count for the current window***. You use math to estimate the rate. Memory usage is constant (**`O(1)`**)
```
User_123_Counter: {
  "Previous_Window_Count": 600,000,
  "Current_Window_Count":  400,000
}
--> TINY MEMORY footprint (Bytes per user)
```

## Comparison of algorithms

|Algorithm|Best Use Case   |Pros                                                         |Cons                                                                               |
|---------|----------------|-------------------------------------------------------------|-----------------------------------------------------------------------------------|
|Token Bucket|General APIs. (e.g., Amazon API Gateway).|Easy to implement; **allows bursts** (good for user experience). |Tuning the burst size can be tricky.                                               |
|Leaky Bucket|**Protecting internal services**. (e.g., Writing to a legacy database).|**Prevents system overload** by enforcing a strict constant rate.|Can cause request lag; drops bursts that the system might have been able to handle.|
|Fixed Window|Low-priority limits|Very low memory/CPU usage                                   |**Spikes at window edges can overload the system (2x limit)**                         |
|Sliding Log|**High-precision, low-volume** (e.g., "Max 5 failed logins per minute").|100% accurate rolling window.                                |**Memory usage is too high for high-throughput APIs** (DDoS protection)               |
|Sliding Counter|**High-scale, precise APIs. (e.g., Cloudflare, Rate limiting proxies)**|**Best balance of accuracy and memory efficiency**              |**Slightly more complex to implement**                                                |
