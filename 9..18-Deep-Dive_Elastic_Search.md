# Deep Dive: Elastic search

## Table of Contents

- [Elastic search](#elastic-search)
  - [What is it?](#what-is-it)
    - [Basic Concepts](#basic-concepts)
    - [The Core Data Units](#the-core-data-units)
  - [3. The Infrastructure or Physical Layer](#3-the-infrastructure-or-physical-layer)
  - [How is ElasticSearch read fast?](#how-is-elasticsearch-read-fast)
  - [Deep dive into Bitsets and Compression](#deep-dive-into-bitsets-and-compression)
    - [Bitsets for filters](#bitsets-for-filters)
    - [FST - Finite State Transducer](#fst-finite-state-transducer)
  - [Where does ElasticSearch fit in a distributed system?](#where-does-elasticsearch-fit-in-a-distributed-system)
  - [Real-world use cases for Elasticsearch](#real-world-use-cases-for-elasticsearch)
  - [When to avoid Elasticsearch](#when-to-avoid-elasticsearch)

## What is it?

Elastic search is an **open source analytics** and **full-text search** engine.

### Basic Concepts

Think of Elasticsearch as **"Google for your own database."** It is a *search engine* that stores data in a way that makes it *incredibly fast to find*.

**SQL vs. Elasticsearch**: If you are familiar with a standard relational database (like MySQL or Postgres), here is the direct translation:

|SQL Concept (Relational)|Elasticsearch Concept (Search Engine)|Description                   |
|------------------------|-------------------------------------|------------------------------|
|Database                |**Cluster**                              |The entire system / group of servers.|
|Table                   |**Index**                                |A logical collection of similar data (e.g., "Users", "Products").|
|Row                     |**Document**                             |A single data entry, stored as JSON.|
|Column                  |**Field**                                |A specific key in the JSON (e.g., "age": 30).|
|Schema                  |**Mapping**                              |The definition of data types (e.g., age is an Integer).|

```json
// POST /products/_doc/101 (Creates an ES "document")
{
  "name": "Red Running Shoes",   // Field
  "price": 49.99,                // Field
  "is_active": true              // Field
}
```

### The Core Data Units

1. **The Document** (The Atom): Everything starts with a Document. It is a JSON object. Unlike SQL rows, documents are flexible (NoSQL)
```js
// Example: A single "Product" in your inventory.
{
  "id": "101",
  "name": "Red Running Shoes",
  "price": 50.00,
  "tags": ["sports", "running", "cheap"]
}
```

2. **The Index** (The Logical Container)
	- You don't just throw documents into a pile. You ***group them*** into an Index
	- Ex: You would create an index called `products` for inventory
	- Ex: You would create an index called `logs-2025` for server logs
	- *Crucial Note*: When you search, you *search against an Index* (e.g., `GET /products/_search`)

## 3. The Infrastructure or Physical Layer


This is where Elasticsearch shines for System Design. It is ***distributed by default***

1. **The Node** (The Server): A Node is a single running instance of Elasticsearch (usually one physical server or VM)
	- It has CPU, RAM, and Disk
	- Nodes find each other to form a **Cluster**
2. **Sharding** (The Scaling Secret)
	- Imagine your products index has 1 Billion documents (5 Terabytes)
	- *Problem*: You cannot fit 5TB on a single Node's hard drive. Even if you could, searching it would be too slow for one CPU
	- Solution: Elasticsearch *slices the **Index** into pieces* called **Shards**

**How Sharding Works**:
- You say: "Create Index products with 3 Shards"
- Elasticsearch takes the 1 Billion docs and puts ~333 million in Shard A, ~333 million in Shard B, etc.
- *Benefit*: You can put Shard A on Node 1, Shard B on Node 2, and Shard C on Node 3. Now you are ***using 3 servers to search one index in parallel***

3. **Replication** (The Safety Net)
	- Hardware fails. If Node 1 catches fire, you lose Shard A (and 33% of your data)
	- Solution: You tell Elasticsearch: "I want 1 Replica"
	- Elasticsearch creates a Copy of Shard A (called a Replica Shard) and puts it on a different node (e.g., Node 2)
	- Rule: ***A Primary Shard and its Replica never live on the same Node***

**High Availability setup**:Here is the complete picture of a Cluster with 2 Nodes hosting 1 Index (split into 2 Primary Shards and 2 Replicas):
```
      USER QUERY: "Find Red Shoes"
                  |
                  v
       +-----------------------+
       |   ELASTICSEARCH       |
       |      CLUSTER          |
       +-----------------------+

 +---------------------+       +---------------------+
 | NODE 1              |       | NODE 2              |
 +---------------------+       +---------------------+
 |                     |       |                     |
 | [ P0 ] Primary      | <---> | [ R0 ] Replica      |
 | Shard 0             | Sync  | Shard 0 (Backup)    |
 | (Contains Docs 1-50)|       |                     |
 |                     |       |                     |
 |                     |       |                     |
 | [ R1 ] Replica      | <---> | [ P1 ] Primary      |
 | Shard 1 (Backup)    | Sync  | Shard 1             |
 |                     |       | (Contains Docs 51-99|
 |                     |       |                     |
 +---------------------+       +---------------------+
```

## How is ElasticSearch read fast?

Elasticsearch is fast because it *cheats*

When you search a standard SQL database for a word like "banana" inside a text column, the database acts like a human reading a book page-by-page to find the word. It s*cans every single row (Full Table Scan)*. This is *slow* (`O(N)`)

**Elasticsearch** uses a data structure called an **Inverted Index**
1. This acts like the ***Index at the back of a textbook***. It *doesn't scan the book*; it looks up the word in the back and tells you exactly which page number (`Document ID`) to go to
2. This is incredibly fast (`O(1)` or `O(log N)`)

The **"Inverted Index" Algorithm** Explained:
- Imagine we have two documents:
	- Doc 1: "The quick brown fox"
	- Doc 2: "The quick blue fox"
- Elasticsearch takes these documents and breaks them into an index that looks like this:
```
TERM (Word)   |  DOC LIST (Where it appears)
----------------|-----------------------------
  blue          |  [ 2 ]
  brown         |  [ 1 ]
  fox           |  [ 1, 2 ]
  quick         |  [ 1, 2 ]
  the           |  [ 1, 2 ]
```
- The Search Algorithm (Intersection): If you search for: "quick" AND "blue"
	- ES looks up "quick" --> List `[1, 2]`
	- ES looks up "blue" --> List `[2]`
	- ES runs a *Set Intersection algorithm* (finding common numbers):
		- `[1, 2] INTERSECT [2] = [2]`
	- Result: Doc 2. It never actually "read" the documents. *It just did math on the lists of IDs*

**Bitsets & Compression**

To make this even faster, Elasticsearch (*via Apache **Lucene***) doesn't just store lists of numbers; ***it often compresses them using Bitsets and FSTs (Finite State Transducers)***.
- **Bitsets**: Instead of storing `[1, 2, 5]`, it might store a binary sequence `11001`. Computers can perform `AND / OR` operations on binary bits efficiently at the CPU level
- **FST**: This compresses the "Terms" (the words) so heavily that the ***entire index often fits into the RAM*** (Memory). If the index is in RAM, there is ***zero Disk I/O***, making the search instant.

## Deep dive into Bitsets and Compression

### Bitsets for filters

Problem: You want to find all documents where `status = "active".` Naive Way: Store a list of document IDs: `[1, 2, 5, 10, ... 99999]`. This ***eats up memory*** (Integers are `32-bits`)
- *The Bitset Way*: Imagine a **long array of bits (0 or 1)**. The *index of the bit corresponds to the Document ID*
```
0 = False (Not active)
1 = True (Active)
```

Example:
- Doc 1: Active
- Doc 2: Active
- Doc 3: Inactive
- Doc 4: Inactive
- Doc 5: Active

Memory View:
```
Doc IDs:  1  2  3  4  5
Bitset:  [1, 1, 0, 0, 1]
```

Why it wins:
- **Tiny**: A bit is 1 bit. An Integer is 32 bits. ***You save 32x memory***
- **Fast Math**: To filter "Active Users" AND "From USA", CPU just does a ***bitwise AND*** on two bitsets. It's lightning fast!

### FST - Finite State Transducer

Problem: Elasticsearch needs to keep the ***"Term Dictionary"*** (the list of all unique words in your data) in RAM so searches are fast. 
- Naive Way: Store strings in a list: `["cat", "cats", "car", "cars"]`
	- Issue: cat, cats, car all share "ca". Storing "ca" 4 times is wasteful!
- The FST Way: An FST is like a ***Trie (Prefix Tree) but smarter***
	- ***It compresses the data by sharing both prefixes (start of word) and suffixes (end of word)***	

Example: Storing cat, cats, car, cars:
```
Trie:
       (Start)
          |
          c
          |
          a
        /   \
       t     r
      / \   / \
     $   s $   s
         |     |
         $     $

FST:
- Like a Trie but prefixes and suffixes are stored as a shared node!
- Below: Notice how the branches merge back together
- The suffix "s" is stored only once

       (Start)
          |
          c
          |
          a
        /   \
       t     r
      / \   / \
     $   \ /   $
          s
          |
          $
```

Why it wins:
1. **Massive Compression**: It maps terms (Key) to their location on disk (Value) using 10-20x less memory than a HashMap
2. **RAM Efficient**: This allows Elasticsearch to hold index structures for billions of documents in memory on a standard server

## Where does ElasticSearch fit in a distributed system?

In a well-designed system, *Elasticsearch is never the primary source of truth (the Producer)*. It is a **Consumer** of data.

You typically use the **"Sidecar Pattern"**. ***Your application writes to a safe database (SQL) first, and then that data flows "downstream" into Elasticsearch***
```
(1) THE SOURCE              (2) THE PIPE                (3) THE CONSUMER
   +----------------+        +------------------+         +-----------------+
   |  Primary DB    |        |  Message Queue   |         |  Elasticsearch  |
   |  (Postgres)    | -----> |  (Kafka)         | ------> |  (Search)       |
   +----------------+        +------------------+         +-----------------+
           |                           ^                           ^
           | (Change Data Capture)     |                           |
           +---------------------------+                           |
             Tool: Debezium                                    Tool: Logstash
                                                               or Custom Consumer
```
How the "Consume" flow works:
- Event: A user updates their profile picture
- Commit: The App saves this to Postgres (Reliable, ACID compliant)
- **Capture: A background process (like Debezium) sees the change in Postgres and pushes an event `UserUpdated` to Kafka**
- Consume: A consumer service (or Logstash) picks up the message from Kafka
- Index: The consumer formats the data as `JSON` and performs a `PUT` request to Elasticsearch.

Why design it this way?
- **Resilience**: *If Elasticsearch crashes (which happens), Kafka holds the messages*. When ES restarts, it "replays" the messages to catch up.
- **Performance**: Your main App doesn't wait for the search index to update. It returns "Success" to the user as soon as Postgres is done

Same example but an Illustration using data flow:
```
User -> [ API Service ]
             |
             v
       (1) Write to DB (Source of Truth)
        [ SQL DB ]
             |
        (2) Change Data Capture (CDC)
             |
             v
        [ Kafka Queue ]  <-- buffers updates
             |
        (3) Consumer Service (Ex: Logstash)
             |
             v
      [ Elasticsearch ]  <-- Updated asynchronously
```

## Real-world use cases for Elasticsearch

Here are the top 4 real-world scenarios where ES is the standard answer.

1. **The "Fuzzy" Store Search (E-Commerce)**
- Interview Question: "Design Amazon" or "Design Netflix"
- The Problem: Typos: The user types "ipone" but wants "iPhone"
	- Relevance: If I search "Red Running Shoes", a product named "Red Nike Running Shoes" (3 matches) should appear before "Red Dress" (1 match). SQL cannot do this "Relevance Scoring."
	- Facets: The sidebar that counts items: "Nike (42), Adidas (12)"

The ES Solution:
- **Fuzzy Matching**: ES uses **"Edit Distance" algorithms** to correct "runing" to "running"
- **BM25 Algorithm**: ES automatically ***calculates a score for every document*** based on ***how well it matches the query*** and sorts by that score
- **Aggregations**: ES calculates those sidebar counts (Facets) in the same query as the search results
```
User Query: "ipone"
       |
       v
[ Elasticsearch ]
   1. Corrects typo -> "iphone" (Fuzzy matching)
   2. Finds 1,000 docs
   3. Ranks by Score (Matches title? description?) (BM25 Algorithm)
       |
       v
Returns:
- iPhone 13 (Score: 10.5)
- iPhone Case (Score: 5.2)
```

2. **Location-Based Search (Geo-Spatial)**
- Interview Question: "Design Yelp", "Design Tinder", or "Design Uber"
- The Problem:
	- You need to combine **Text + Location**
		- Ex: "Find Mexican food (Text) that is 4 stars (Filter) and within 5 miles of me (Geo)."

The ES Solution:
- Postgres (PostGIS) is good at location, but bad at "Mexican food" text relevance
- ES is great at both. **It uses a `geo_point` data type**. It can **efficiently filter by a bounding box (lat/long)** AND **score by text relevance simultaneously (BM25 algo)**.

```
THE CHALLENGE: Composite Query
   "Mexican food" (Text Relevance)
   + "4 Stars" (Structured Filter)
   + "Within 5 miles" (Geo Filter)
               |
               | (Single API Call)
               V
+-----------------------------------+
|      ELASTICSEARCH NODE           |
|                                   |
|  +--- Document Structure ---+     |
|  | "name": "El Pastor"      |     |
|  | "tags": ["mexican"]      |     |
|  | "rating": 4.5            |     |
|  | "location": {            |     |
|  |   "lat": 40.71,          | <---|--- Stored as `geo_point` type
|  |   "lon": -74.00          |     |
|  | }                        |     |
|  +--------------------------+     |
|                                   |
|       PROCESSING ENGINE           |
|                                   |
| [ Text Scoring (BM25 Algo) ]      | <-- "How relevant is 'Mexican'?"
|              +                    |     (SIMULTANEOUS)
| [ Geo Filtering (Bounding Box) ]  | <-- "Is lat/lon inside 5 mile radius?"
|                                   |
+-----------------------------------+
               |
               V
      FINAL RANKED RESULTS
(Sorted by combined text score & distance)
```

System Design Tip: 
- For Tinder, you use ES to query: `Age: 25-30 AND Gender: F AND Location: < 10km AND Interests: Hiking`
- ES handles this "multi-dimensional" filtering faster than any SQL database

3. **Observability & Log Analysis**
- Interview Question: "Design a Log Monitoring System (like Datadog/Splunk)"
- The Problem: Ingest 100,000 logs per second & search on them (The logs can be unstructured)
- ***Search for unstructured errors***:
	- Example: Search for "Connection failed at port 8080 caused by timeout"
- SQL dies because every log message is different; you can't normalize this into columns!

The ES Solution: 
- **Tokenization**: ES breaks that long error string into individual words: ["connection", "failed", "port", "8080"]. You can now search for failed AND 8080 instantly!

**Write Speed**: ES is designed to ***swallow massive amounts of text data quickly*** (using an **"Inverted Index"** which is **append-friendly**)

```
Raw Log Input
"Connection failed at port 8080"
          |
          v
 [ ELASTICSEARCH TOKENIZER ] (Breaks it apart)
          |
          +------+-----------+--------+------+
                 |           |        |      |
Inverted Index: ["connection", "failed", "port", "8080"]
                 ^           ^
                 |           |
Query:   "Find logs with 'failed' AND '8080'"
Result:  Instant Match!
```

4. **Smart Autocomplete (Typeahead)**
- Interview Question: "Design Google Search Autocomplete"
- The Problem:
	- User types "San F...".
	- You need to return "San Francisco" instantly (< 50ms).

The ES Solution: **Edge N-Grams**: When you save (write) "San Francisco", ES ***automatically saves partial versions***: `s`, `sa`, `san`, `san f`.
- When the user types "San F", it's an *exact match* for one of these *partial* tokens

**Trade-off**: 
- ***Redis is faster for simple prefixes***
- BUT, ***ES allows for fuzzy autocomplete*** (e.g., matching "Sn Fransisco" to "San Francisco")

```
INDEXING (Writing Data)
Document: "San Francisco"
          |
          v
[ ES EDGE N-GRAM ANALYZER ]
          |
Stores:  [ s, sa, san, san f, san fr, ... san francisco ]
            \   /
             \ /
              V
SEARCHING (User types)
User: "San F" -> Exact Match found in index!
Result: "San Francisco" (returned in < 50ms)
```

## When to avoid Elasticsearch

When to AVOID ES in System Design? Don't use it as the Primary System of Record for:

1. **Payments/Banking**: ES is *"Eventually Consistent".* You might write a payment and not see it for 1 second. That's unacceptable for money!
2. **Relationships**: If you need "Get User's Friends' Posts" (*Graph data*), ES is bad! Use SQL or a Graph DB.
