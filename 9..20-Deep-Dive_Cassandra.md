# Deep Dive: Cassandra

## Table of Contents

- [Deep Dive: Cassandra](#deep-dive-cassandra)
  - [High-Level Summary](#high-level-summary)
  - [Core Architecture](#core-architecture)
    - [Replication](#replication)
    - [Gossip Protocol](#gossip-protocol)
  - [The Read & Write Path Internals](#the-read-write-path-internals)
    - [The Write Path - Optimized for Speed](#the-write-path-optimized-for-speed)
    - [The Read Path](#the-read-path)
  - [Data model within Cassandra](#data-model-within-cassandra)
  - [Tunable Consistency](#tunable-consistency)
  - [Data Modeling - Query-Driven Design](#data-modeling-query-driven-design)
  - [When to Use Cassandra](#when-to-use-cassandra)
  - [When NOT to use Cassandra](#when-not-to-use-cassandra)
  - [Example system design diagram](#example-system-design-diagram)


## High-Level Summary

Apache Cassandra is a ***distributed, wide-column NoSQL database*** designed for ***high write throughput, high availability, and horizontal scalability***

Origin: Created by Facebook (Avinash Lakshman) to power Inbox Search. It combines the replication model of Amazon Dynamo with the data model of Google BigTable

CAP Theorem: It is an **AP system (Availability + Partition Tolerance)**. It sacrifices strict consistency for the ability to always accept writes, even if some nodes are down

## Core Architecture

Cassandra creates a ***"masterless" ring architecture*** where all nodes are equal
- ***Consistent Hashing*** (The Ring): Data is partitioned using a Partition Key (e.g., `user_id`)
	- The key is hashed to a numeric token (e.g., `0` to `2^64-1`)
	- Nodes are arranged in a *ring*; each node owns a range of tokens
	- Virtual Nodes (vnodes): Instead of one big range, physical machines own many small, non-contiguous ranges (vnodes) on the ring.
		- This ensures data is *evenly distributed and makes adding/removing nodes faster* (no massive reshuffling)

```
     (Key: "User_123")
             |
             v
      [ Hash Function ]  ---> Token: 55
             |
             +----------------------------+
                                          |
        Token Range: 0-25        Token Range: 26-50
       +-----------------+      +-----------------+
       |     Node A      |<-----|     Node B      |
       +-----------------+      +-----------------+
              ^                          ^
              |                          |
        Token Range: 76-100      Token Range: 51-75 <--- (Token 55 lands here)
       +-----------------+      +-----------------+
       |     Node D      |----->|     Node C      |
       +-----------------+      +-----------------+

Key Concept: The Ring
* Node C "owns" the token range 51-75.
* If Node C is the "Coordinator," it also knows exactly
  where to send Replicas (e.g., to Node D and Node A)
  based on the Replication Strategy.
```

### Replication

Replicas are placed on the next `N` nodes clockwise in the ring
- Snitch/Strategy: In production, you use `NetworkTopologyStrategy `to ensure replicas are placed on different physical racks or data centers to survive hardware failures

### Gossip Protocol

Nodes ***do not*** have a central master (like Zookeeper)
	- Instead, they "gossip" with random peers every second to share state (who is up, who is down)

## The Read & Write Path Internals

Cassandra uses an LSM Tree (Log-Structured Merge Tree) structure

### The Write Path - Optimized for Speed

Cassandra writes are ***incredibly fast*** because they are primarily *"append-only"*

Commit Log (Disk): 
- The write is immediately appended to a file on disk for durability (crash recovery)

Memtable (RAM): 
- The data is written to an in-memory sorted structure

SSTable (Disk): 
- When the Memtable is full, it is flushed to disk as an immutable SSTable (Sorted String Table)

Compaction: 
- Since SSTables are immutable, updates/deletes just write new data. A background process merges these files later to reclaim space (and remove "tombstones" which mark deleted data)

```
       +--------+
       | Client |
       +----+---+
            | (1) Write Request
            v
    +-------+--------+
    |   Cassandra    |
    |      Node      |
    +-------+--------+
            |
            +---------------------------+
            |                           |
            v                           v
  +=========+==========+      +---------+----------+
  |    Disk (HDD/SSD)  |      |      RAM (Memory)  |
  |                    |      |                    |
  |  (2) Commit Log    |      |  (3) Memtable      |
  |  [Append Only]     |      |  [Sorted Tree]     |
  |  (Durability)      |      |  (Speed)           |
  +====================+      +---------+----------+
                                        |
                                        | (4) Flush (When full)
                                        |
                                        v
                              +=========+==========+
                              |    Disk (HDD/SSD)  |
                              |                    |
                              |   (5) SSTables     |
                              |   [Immutable Data] |
                              |   File 1: [Data..] |
                              |   File 2: [Data..] |
                              +====================+

"Writes hit the Commit Log (for safety) and Memtable (for speed) simultaneously."

"Once the Memtable is full, it flushes to an SSTable on disk. 
We never edit SSTables; we only write new ones."
```

***Compaction diagram***:
```
Before Compaction:             After Compaction:
(Many small files,             (One large, clean file)
 potential duplicates)

+------------+                  +----------------+
| SSTable 1  |                  | New SSTable    |
| Key: A (v1)|                  |                |
+------------+      ====>       | Key: A (v2)    |
                                | Key: B (v1)    |
+-------------+                 | Key: D (v1)    |
| SSTable 2   |                 +----------------+
| Key: A (v2) |
| Key: B (v1) |                 * Key C (deleted) is
| Key: C (del)|                  purged.
+-------------+                 * Key A (v1) is
                                 discarded for v2.
```
### The Read Path

Reads are *slower than writes* because data might be *spread across multiple SSTables*

Memtable Check: 
- Is the data in memory?

Bloom Filter:
- A probabilistic data structure checks if the data might be in a specific SSTable (skips files that definitely don't have it)

SSTable Scan:
- Scans the remaining SSTables and merges the results to find the latest version (based on timestamp)

Read Repair: 
- If replicas disagree (e.g., Node A has v1, Node B has v2), the coordinator returns the latest data to the client and issues a background write to fix the stale node

```
Why reads are slower and how Cassandra optimizes them using Bloom Filters:

       +--------+
       | Client |
       +----+---+
            | (1) Read Request ("User_123")
            v
    +-------+--------+
    | Memtable       | (2) Check Memory First
    | (RAM)          |     (Fastest)
    +-------+--------+
            |
            | (If not found, go to Disk)
            v
    +=======================================+
    |             SSTable (Disk)            |
    |                                       |
    |  [ Bloom Filter ] <--- (3) "Do I have this Key?"
    |   (Probabilistic)      (Returns: No / Maybe)
    |          |                            |
    |          | (If Maybe)                 | (If No)
    |          v                            v
    |  [ Partition Index ]               (Skip File)
    |   (Offset lookup)                     |
    |          |                            |
    |          v                            |
    |      [ Data ]                         |
    |                                       |
    +=======================================+
```

## Data model within Cassandra

At a high level, Cassandra looks like a standard SQL table with rows and columns. However, under the hood, it is actually a **multidimensional sorted map**

In a Relational DB (SQL), every row must have the same columns. In Cassandra, that is effectively true for the schema, but physically, it functions as a ***Nested Map***.

The Formula: `Map<PartitionKey, SortedMap<ClusteringKey, Columns>>`

```
Partition Key: Chat_Room_ID (This groups all messages for one room together on one server)

Clustering Key: Message_ID (This sorts the messages by time inside that room)

Row Key (Partition Key)
      |
      v
+-----------------------+
|  "Room_101"           | <--- The "Partition" (Lives on 1 Node)
+-----------------------+
      |
      +---> [ Clustering Key: 1001 ] (Time: 10:00 AM)
      |         |
      |         +-- "User": "Alice"
      |         +-- "Text": "Hello!"
      |
      +---> [ Clustering Key: 1002 ] (Time: 10:01 AM)
      |         |
      |         +-- "User": "Bob"
      |         +-- "Text": "Hi Alice"
      |
      +---> [ Clustering Key: 1003 ] ...
```

When you use **CQL (Cassandra Query Language)**, it looks deceptively like a SQL table

```
CREATE TABLE chat_messages (
    room_id text,          -- Partition Key
    message_id timeuuid,   -- Clustering Key
    user_id text,
    message_text text,
    PRIMARY KEY ((room_id), message_id)
) WITH CLUSTERING ORDER BY (message_id DESC);
```
```
+---------------------+---------------------------------------------------+
|  PARTITION KEY      |                  CLUSTERING COLUMNS               |
|  (Determines Node)  |             (Determines Sort Order on Disk)       |
+=====================+======================+=============+==============+
|     room_id         |     message_id       |   user_id   | message_text |
+---------------------+----------------------+-------------+--------------+
|                     |  uuid-1 (10:05 PM)   |    Dave     |  "Bye!"      |
|   "Room_101"        +----------------------+-------------+--------------+
|                     |  uuid-2 (10:02 PM)   |    Bob      |  "Sup?"      |
|                     +----------------------+-------------+--------------+
|                     |  uuid-3 (10:00 PM)   |    Alice    |  "Hello"     |
+---------------------+----------------------+-------------+--------------+
|   "Room_102"        |  uuid-4 (09:00 AM)   |    Eve      |  "Meeting?"  |
+---------------------+----------------------+-------------+--------------+
```

**Clustering Key (The "Sorting" mechanism):**

Inside the node containing `Room_101`, the data is stored sorted by `message_id`.

This makes ***"Range Scans"*** (e.g., "Give me the last 50 messages") ***incredibly fast*** because the disk head just seeks to the start and reads sequentially

Columns: The actual data values (Key-Value pairs essentially)

## Tunable Consistency

You don't have to choose between Consistency and Availability globally; ***you choose per request***

|Level |Description     |Use Case                      |
|------|----------------|------------------------------|
|ONE   |Only 1 replica needs to ack.|Highest Speed, lowest consistency (e.g., IoT logs, view counts).|
|QUORUM|Majority (N/2+1) must ack.|Balanced. Guarantees strong consistency if R+W>N.|
|ALL   |All replicas must ack.|Highest Consistency, lowest availability (one down node fails the request).|

***Standard Pattern***: `Replication Factor = 3`, `Write = QUORUM (2)`, `Read = QUORUM (2)`. This guarantees you always read the latest write!

```
"R + W > N" (Quorum formula)

Scenario: Replication Factor (N) = 3
          Write Consistency (W)  = Quorum (2)
          Read Consistency  (R)  = Quorum (2)

      +--------+               +--------+
      | Write  |               |  Read  |
      | Request|               | Request|
      +----+---+               +----+---+
           |                        |
           |                        |
    /------+------\          /------+------\
    |      |      |          |      |      |
    v      v      v          v      v      v
+---+--+ +---+--+ +---+--+ +---+--+ +---+--+
| Node | | Node | | Node | | Node | | Node |
|  A   | |  B   | |  C   | |  A   | |  B   |
+------+ +------+ +------+ +------+ +------+
   ^        ^                  ^        ^
   |        |                  |        |
   +--------+                  +--------+
    Written                     Read from
    to A & B                    A & B

    Result: Intersection!
    Node A and Node B have the latest data.
    Even if Node C was down, the Read sees the data from A/B.
```

## Data Modeling - Query-Driven Design

This is the biggest trap for engineers coming from SQL

**No Joins**: Cassandra does not support joins!

**Denormalization**: You must design your tables based on queries, not entities!

- **SQL Approach**: Users table and Posts table. Join on `user_id`
- **Cassandra Approach**: A `PostsByUserId` table that contains the *User's data duplicated inside every Post row*

Primary Key Breakdown:
1. **Partition Key**: Determines *which node holds the data*
2. **Clustering Key**: Determines *how data is sorted on disk inside that node* (e.g., sort posts by `timestamp DESC`)

## When to Use Cassandra

Use it when:
1. **Writes > Reads**: You have massive ingestion (e.g., `>100k writes/sec`)
2. **Time Series / Logs**: Sensor data, chat history, activity feeds

**Global Scale**: You need *multi-datacenter replication* out of the box

## When NOT to use Cassandra

Do NOT use it when:
1. ACID Transactions: You need strong financial consistency (use SQL/NewSQL).
2. Ad-hoc Queries: You *don't know your query patterns upfront* (Cassandra requires modeling for specific queries)
3. Dynamic Data: You need to frequently update/delete data (*tombstones* can slow down performance)

## Example system design diagram

The "High-Scale Ingestion" Pattern (Simple): Use this for Sensor Data (IoT), User Activity Logs, or Metric Collection
- Why here? The "Ingestion Service" needs to dump data fast (100k+ writes/sec). Cassandra's write speed is superior to SQL here
- **NO CACHE!** Often you ***don't need a cache*** in front of Cassandra for write-heavy workloads because the "Memtable" (internal RAM buffer) acts as a ***write-back cache***
```
+--------+       +-----------+
|  IoT   | ----> |   Load    |
| Device |       | Balancer  |
+--------+       +-----+-----+
                       |
        +--------------+--------------+
        |                             |
+-------v-------+             +-------v-------+
|   Ingestion   |             |   Ingestion   |
|   Service     |             |   Service     |
| (Stateless)   |             | (Stateless)   |
+-------+-------+             +-------+-------+
        |                             |
        | (1) High Velocity Writes    |
        |     (Fire & Forget)         |
        v                             v
+===================================================+
|               CASSANDRA CLUSTER                   |
|  +-------------+  +-------------+  +-----------+  |
|  | Node A      |  | Node B      |  | Node C    |  |
|  | (Partition  |  | (Replica 1) |  | (Replica 2)| |
|  |  Key: DevID)|  |             |  |           |  |
|  +-------------+  +-------------+  +-----------+  |
+===================================================+
```

