# Kafka Deep Dive

## Table of Contents

- [Kafka Deep Dive](#kafka-deep-dive)
  - [Terms used by Kafka](#terms-used-by-kafka)
  - [Topics and Partitions](#topics-and-partitions)
  - [How Kafka works](#how-kafka-works)
  - [Handling hot partitions in Kafka](#handling-hot-partitions-in-kafka)
  - [Performance Optimization of Kafka](#performance-optimization-of-kafka)
  - [Drawback of kafka](#drawback-of-kafka)
    - [Dead Letter Queues](#dead-letter-queues)
  - [Kafka vs other tools](#kafka-vs-other-tools)
  - [Example system design diagram](#example-system-design-diagram)


Kafka is an **event streaming platform**. It allows different parts of a system to talk to each other ***asynchronously*** by writing and reading "**events**" (messages) to/from a central log

It acts as a buffer between systems that produce data (**Producers**) and systems that process data (**Consumers**), allowing them to ***scale independently***

## Terms used by Kafka

1. **Message (Record)**: The data being sent (e.g., "Goal scored by Messi"). It consists of a `Key`, `Value`, `Timestamp`, and `Headers`
2. **Producer**: The application sending messages
3. **Consumer**: The application reading messages
4. **Broker**: A *single* Kafka server. A Kafka Cluster is made of multiple brokers working together.
5. **Topic**: A logical category for messages (e.g., "soccer-updates", "user-logins")
6. **Partition**: The *actual "physical" log file* on a server. ***A Topic is split into multiple Partitions to scale***
	- Analogy: 
		- A Topic is like a filing cabinet
		- The Partitions are the individual drawers. You can have different drawers on different servers
7. **Offset**: A *unique ID sequence number* for a message inside a partition (e.g., `Message 0`, `Message 1`, `Message 2`)
8. **Consumer Group**: A *group of consumers working together to read from a topic*. Kafka ensures that ***each partition is read by only one consumer in the group, preventing duplicate processing***

```
       Topic: "soccer-updates"
      (Split into 3 Partitions)

       +-----------------------+
       | Partition 0 (Broker 1)|  <-- Consumer A (Reads Part 0)
       | [Msg 0] [Msg 1] [Msg 2] ...
       +-----------------------+

       +-----------------------+
       | Partition 1 (Broker 2)|  <-- Consumer B (Reads Part 1)
       | [Msg 0] [Msg 1] ...
       +-----------------------+

       +-----------------------+
       | Partition 2 (Broker 3)|  <-- Consumer C (Reads Part 2)
       | [Msg 0] [Msg 1] [Msg 2] [Msg 3] ...
       +-----------------------+
```

## Topics and Partitions 

Why use Topics and Partitions? Answer: **Scaling**

A Topic (e.g., "Web-Clicks") is *too big* for one server. So, Kafka splits the Topic into Partitions and spreads them across different servers (**Brokers** i.e physical computers)
```
TOPIC: "Web-Clicks"
(Split into 3 Partitions across 3 Servers)

Server 1 (Broker)        Server 2 (Broker)        Server 3 (Broker)
+------------------+     +------------------+     +------------------+
| Partition 0      |     | Partition 1      |     | Partition 2      |
| [0][1][2][3]...  |     | [0][1][2]...     |     | [0][1][2][3]...  |
+------------------+     +------------------+     +------------------+
```
*Why do this?*
- If you only had Server 1, you could only handle 1,000 clicks/sec.
- With 3 servers, you can handle 3,000 clicks/sec
- The Key: The Producer decides which partition to use
	- Key = "User A" -> Hashes to Partition 0
	- Key = "User B" -> Hashes to Partition 1
- Result: All actions by User A are in order inside Partition 0

**Understanding topics and partitions with a real world analogy**

Here is the simplest analogy: **The Highway**

*The Concept*
- **Topic**: The ***Destination*** (e.g., "Traffic heading to London"). It groups all the cars (messages) going to the same place
- **Partition**: The ***Lanes*** on that highway.

Why do we need Partitions? ***If you only have one lane, traffic jams happen***. If you open 3 lanes (Partitions), 3 times as many cars can drive at once

*The Golden Rule*
Ordering: You can't pass a car in your own lane (***Order is guaranteed within a Partition***). But Lane 2 might move faster than Lane 1 (***No order guarantee across the whole Topic***)

In the real world, the "Lanes" (*Partitions*) are *split* across different computers (**Brokers**). This is why *Kafka creates **massive scale**â€”it uses the CPU and Disk of multiple machines at once*

```
TOPIC: "Orders" (The Highway)
=============================

------------------------------------------
Partition 0 (Lane 1)   [Car] [Car] [Car]  ->  (Consumer A works here)
------------------------------------------
Partition 1 (Lane 2)   [Car] [Car] [Car]  ->  (Consumer B works here)
------------------------------------------
Partition 2 (Lane 3)   [Car] [Car] [Car]  ->  (Consumer C works here)
------------------------------------------
```

```
    Topic: "Web_Clicks" (3 Partitions)
    ----------------------------------

    +---------------------+           +---------------------+
    |      SERVER A       |           |      SERVER B       |
    | (Kafka Broker 1)    |           | (Kafka Broker 2)    |
    +---------------------+           +---------------------+
    |                     |           |                     |
    |  [ Partition 0 ]    |           |  [ Partition 2 ]    |
    |  [0][1][2]...       |           |  [0][1][2]...       |
    |                     |           |                     |
    +---------------------+           +---------------------+
              ^
              |
    +---------------------+
    |      SERVER C       |
    | (Kafka Broker 3)    |
    +---------------------+
    |                     |
    |  [ Partition 1 ]    |
    |  [0][1][2]...       |
    |                     |
    +---------------------+
```

**Understanding topics and partitions with a practical example**

Ride Sharing App (like Uber):
1. The Real Message (Event): A "Ride Requested" event. This is the data payload inside the Kafka message
```json
{
  "event": "RIDE_REQUESTED",
  "user_id": "12345",
  "location": "New York",
  "timestamp": 1672531200
}
```
2. How Topics & Partitions Help: 
	- Topic: `ride_events` (The category)
	- Partitioning Strategy: We split data by `City` so we can process different cities in parallel
		- Partition 0: Handles New York
		- Partition 1: Handles London
```
       PRODUCER (App API)
            |
            | Sends 4 messages
            v
      TOPIC: "ride_events"
      -------------------------------------------------
      PARTITION 0 (Key: NYC)    PARTITION 1 (Key: LON)
      [Msg1: User 123 (NYC)]    [Msg3: User 789 (LON)]
      [Msg2: User 456 (NYC)]    [Msg4: User 999 (LON)]
            |                         |
            |                         |
            v                         v
      [CONSUMER A]              [CONSUMER B]
      (Processes NYC)           (Processes London)
```
*Why this matters*: If New York has a huge surge in traffic (1 million requests), Partition 0 gets busy. However, Consumer B (London) is completely unaffected and keeps processing London rides smoothly because it is on a different "lane"!

***Understanding Consumer Groups***

**Parallel Processing**: This is what Consumer Groups are great for!
- This is how Kafka handles *massive traffic*. You don't just have one consumer; you have a "Consumer Group". ***Kafka automatically divides the work (Partitions) among the members of the group***

```
KAFKA CLUSTER                          CONSUMER GROUP "Email-Senders"
+---------------+
| Partition 0   | ------------------->  [ Consumer A ] (Processing Part 0)
+---------------+

+---------------+
| Partition 1   | ------------------->  [ Consumer B ] (Processing Part 1)
+---------------+

+---------------+
| Partition 2   | ------------------->  [ Consumer C ] (Processing Part 2)
+---------------+
```

**Rule**: One Partition is consumed by *only one* Consumer in the group
- *Why*? If Consumer A and B both read Partition 0, you would send the user duplicate emails
- *Scaling*: If you add Partition 3, you can add Consumer D to handle it

**Can one broker (computer) have Partitions from DIFFERENT Topics?**

Yes, a single broker usually holds a mix of data from various topics. It is a common behaviour
```
      +-----------------------------+
      |         BROKER 1            |
      |   (The Physical Server)     |
      +-----------------------------+
      |                             |
      |  [ Topic: "User_Logs" ]     |
      |  [ Partition 0        ]     |
      |                             |
      |  -------------------------  |
      |                             |
      |  [ Topic: "Orders"    ]     |
      |  [ Partition 2        ]     |
      |                             |
      +-----------------------------+
```

**Can one broker (computer) have Partitions from a SINGLE Topic?**

YES. A single broker can hold multiple partitions of the same topic.
- Why? This happens if you *have more partitions than brokers*.
- Example: You have 1 Broker, but you created the Topic "Clicks" with 3 Partitions. That one broker must hold all 3

```
      +-----------------------------+
      |         BROKER 1            |
      +-----------------------------+
      |                             |
      |  [ Topic: "Clicks"    ]     |
      |  [ Partition 0        ]     |
      |                             |
      |  [ Topic: "Clicks"    ]     |
      |  [ Partition 1        ]     |
      |                             |
      |  [ Topic: "Clicks"    ]     |
      |  [ Partition 2        ]     |
      |                             |
      +-----------------------------+
      (This broker handles 100% of the load for "Clicks")
```
 
**Key Takeaway**: While possible, ***putting all partitions of one topic on a single broker defeats the purpose of "Distributed" scaling***. Ideally, you want to *spread them out* (e.g., Partition 0 on Broker A, Partition 1 on Broker B) so two CPUs work at once

## How Kafka works

**Writing Data (Producers)**
1. The Key: The producer usually specifies a "Key" (e.g., `GameID: 123`)
2. Hashing: Kafka *hashes* the Key to assign it to a partition *deterministically*
3. Result: All updates for `GameID: 123` will always go to Partition 5. This guarantees strict ordering for that specific game
4. No Key?: If no key is provided, Kafka *Round-Robins* the messages across all partitions (good for load balancing, bad if order matters)

```
     PRODUCER
         |
         |  Msg 1: [Key: 123 | "Player Joined"]
         |  Msg 2: [Key: 123 | "Score +10"]
         v
    +---------+
    | HASHING |  <-- Math: Hash(123) % Total_Partitions = 5
    +---------+
         |
         | (Always routes to Partition 5)
         v
    +------------------------------+
    |           TOPIC              |
    |  +------------------------+  |
    |  | Partition 0            |  |
    |  +------------------------+  |
    |             ...              |
    |  +------------------------+  |
    |  | Partition 5            |  | <--- [Msg 1][Msg 2]
    |  +------------------------+  |      (Strict Order Kept)
    +------------------------------+
```

*Kafka producer example snippet (Kafkajs -- A NodeJS client)*:
```js
// Initialize the Kafka client
const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
})

// Initialize the producer
const producer = kafka.producer()

const run = async () => {
  // Connecting the producer
  await producer.connect()

  // Sending messages to the topic 'my_topic' with keys
  await producer.send({
    topic: 'my_topic',
    messages: [
      { key: 'key1', value: 'Hello, Kafka with key!' },
      { key: 'key2', value: 'Another message with a different key' }
    ],
  })
}
```

**Reading Data (Consumers)**
1. Offsets & Commits: As a consumer reads, it "commits" its offset. This is like a *bookmark*. It tells Kafka, "I have finished processing up to Message #5"
2. Failure: If a Consumer crashes, a new one takes over, *reads the last committed offset* ("Okay, start at #6"), and resumes

```
                     KAFKA TOPIC (Partition 0)
    +-------------------------------------------------------+
    |  Msg 1  |  Msg 2  |  Msg 3  |  Msg 4  |  Msg 5  | ... |
    +-------------------------------------------------------+
       ^          ^          ^          ^
       |          |          |          |
    (Read)     (Read)     (Read)     (Processing...)

            +--------------+
            |  CONSUMER A  |  <-- Currently working on Msg 4
            +--------------+
                   |
                   | (Periodically says: "I'm done with #3")
                   v
            +--------------+
            |  __consumer  |
            |   _offsets   |  <-- "Partition 0: Offset 3" (SAVED)
            +--------------+
```

*Kafka consumer example snippet (Kafkajs -- A NodeJS client)*:
```js
// Initialize the Kafka client
const kafka = new Kafka({
  clientId: 'my-app',
  brokers: ['localhost:9092']
})

// Initialize the consumer
const consumer = kafka.consumer({ groupId: 'my-group' })

const run = async () => {
  // Connecting the consumer
  await consumer.connect()

  // Subscribing to the topic 'my_topic'
  await consumer.subscribe({ topic: 'my_topic' })

  // Consuming messages
  await consumer.run({
    eachMessage: async ({ topic, partition, message }) => {
      console.log({
        value: message.value.toString(),
        key: message.key.toString()
      })
    },
  })
}
```

**Fault Tolerance (Replication)**
1. Leader: The *main copy* of a partition that handles all reads/writes
2. Follower: Passive backups that just copy data from the Leader
3. Replication Factor: A setting (default usually 3) that dictates how many copies exist. If the Leader dies, a Follower is instantly promoted to Leader

Everything flows through the **Leader**. The Followers are just passive backups (like a backup hard drive).
```
             +-----------+
             | PRODUCER  |
             +-----------+
                   |
                   | 1. WRITE (Msg A)
                   v
        +----------------------+
        |      BROKER 1        |
        | [Part 0: LEADER]     | <--- "I am the Boss."
        +----------------------+      (Handles all Reads/Writes)
             /           \
            / (Sync)      \ (Sync)
           v               v
 +-------------------+   +-------------------+
 |    BROKER 2       |   |    BROKER 3       |
 | [Part 0: Follower]|   | [Part 0: Follower]|
 +-------------------+   +-------------------+
      (Backup 1)             (Backup 2)
```

**The Failover** (When Leader Dies): If Broker 1 crashes, Kafka (using *Zookeeper/KRaft* controller) detects it instantly (No data is lost)! *One of the Followers is promoted to become the New Leader*

```
        +----------------------+
        |      BROKER 1        |
        |       (DEAD X)       | <--- Crash!
        +----------------------+

             [ ELECTION ]
  (Who has the most up-to-date data?)
                  |
                  v
 +------------------+   +------------------+
 |    BROKER 2      |   |    BROKER 3      |
 | [Part 0: LEADER] |   | [Part 0: Follower]|
 +------------------+   +------------------+
         ^                       ^
         |                       |
   (New Writes go here)     (Still copying)
```

## Handling hot partitions in Kafka

**Handling "Hot Partitions"**: Imagine one game is the World Cup Final. Millions of events happen. If all those go to Partition 1, that server will crash while others sit idle. This is a *"Hot Partition"*

```
    PRODUCER (Sending 1M events for "WorldCup")
         |
         | (Hash("WorldCup") -> Partition 0)
         v
    +----------------------+
    |     PARTITION 0      |  <-- [CRASHING!] (CPU 100%, Disk Full)
    | [Msg][Msg][Msg][Msg] |      (Millions of events piling up)
    | [Msg][Msg][Msg][Msg] |
    +----------------------+

    +----------------------+
    |     PARTITION 1      |  <-- [IDLE] (0% Usage)
    |       (Empty)        |
    +----------------------+
```

**Solution 1 (Random Salting)**: Add a *random number* to the key (e.g., `GameID:123-1`, `GameID:123-2`). This *spreads the data across partitions*
- Trade-off: **You lose strict ordering** because data is now everywhere

```
    PRODUCER (Appends Random Suffix)
         |
         +-> "WorldCup-1" -> Hash -> Partition 0
         +-> "WorldCup-2" -> Hash -> Partition 1
         +-> "WorldCup-3" -> Hash -> Partition 2
         v
    +-------------+    +-------------+    +-------------+
    | PARTITION 0 |    | PARTITION 1 |    | PARTITION 2 |
    | [Msg A]     |    | [Msg B]     |    | [Msg C]     |
    +-------------+    +-------------+    +-------------+
       (Balanced Load - 33% each)
```

**Solution 2 (Compound Key)**: Use `GameID + UserID` as the key. This splits the load while keeping updates for a specific user ordered

```
PRODUCER (Key = "WorldCup:UserID")
         |
         +-> "WorldCup:UserA" -> Hash -> Partition 0
         +-> "WorldCup:UserB" -> Hash -> Partition 1
         v
    +-------------+             +-------------+
    | PARTITION 0 |             | PARTITION 1 |
    | [UserA: Jump]             | [UserB: Run ]
    | [UserA: Shoot]            | [UserB: Goal]
    +-------------+             +-------------+
     (User A is ordered)         (User B is ordered)

Why this wins: The "World Cup" load is split, 
but individual users (who need their actions processed in order) 
still have their history intact.
```

## Performance Optimization of Kafka

1. **Batching**: Instead of sending 1 message at a time, the producer waits 100ms and sends 50 messages in one packet.
2. **Compression**: Use GZIP or Snappy to shrink the data size before sending.

## Drawback of kafka

*The Bottleneck*: **Head-of-Line Blocking**: This diagram explains the biggest "Con" of Kafka: **Ordering limits**

```
PARTITION 0 (Ordered Queue)
+--------------------------------------------------+
| Msg 1: Account Created                           |
| Msg 2: ERROR! (Consumer crashes processing this) |
| Msg 3: Account Activated                         |
+--------------------------------------------------+
```
- Scenario: You are processing strict financial events
- The Problem: The consumer tries to read Msg 2 and crashes. It restarts, tries Msg 2 again, and crashes again
- The Block: Msg 3 cannot be processed until Msg 2 is resolved. The whole partition is "blocked"
- ***Solution***: You need **"Dead Letter Queues"** (a separate side-queue) to move bad messages out of the way so processing can continue

### Dead Letter Queues

**Concept**: A Dead Letter Queue (DLQ) is a "safety net" topic
- It is a separate Kafka topic where you move messages that your application cannot process after trying multiple times
- Instead of letting one "bad apple" (a corrupted or malformed message) crash your consumer or block the entire pipeline, you push it to the DLQ so your system can continue processing the rest of the valid messages

```
            MAIN TOPIC ("orders")
      +--------------------------------+
      | [Msg A: OK]                    |
      | [Msg B: ERROR] (Bad Data)      | <-- Consumer fails to process this
      | [Msg C: OK]                    |
      +--------------------------------+
             |
             v
      +--------------+
      |   CONSUMER   | (App Logic)
      +--------------+
             |
             | 1. Try... Fail.
             | 2. Retry... Fail.
             | 3. GIVE UP -> Send to DLQ
             v
      DLQ TOPIC ("orders-dlq")
      +--------------------------------+
      | [Msg B: ERROR]                 | <-- Sits here safely for inspection
      +--------------------------------+
             |
             | (Later)
             v
      +--------------+
      | DEBUG TOOL / |
      | DEVELOPER    |
      +--------------+
      1. Alert team ("DLQ isn't empty!")
      2. Fix the bug or fix the data manually
      3. Re-send Msg B to Main Topic
```

**What do you do with the DLQ?**
- You don't just ignore it. You usually set up an alert (e.g., PagerDuty/Slack) when the DLQ size > 0
- A developer then inspects the bad message to decide if it's a *code bug (release a fix)* or *bad data (discard or manually fix and re-inject)*

## Kafka vs other tools

|Tool  |Type            |Best For                      |Ops Effort          |
|------|----------------|------------------------------|--------------------|
|Kafka |Stream          |Massive history & throughput  |High                |
|RabbitMQ|Broker          |Complex routing logic         |Medium              |
|GCP Pub/Sub|Cloud Queue     |Global scale on Google Cloud  |Zero                |
|Amazon SQS|Cloud Queue     |Simple decoupling on AWS      |Zero                |

## Example system design diagram

```
+--------+       +-----------+
| Client | ----> | Load      |
+--------+       | Balancer  |
                 +-----------+
                       |
        +--------------+--------------+
        |                             |
+-------v-------+             +-------v-------+
|  Web Server / |             |  Microservice |
|  Producer A   |             |  Producer B   |
+-------+-------+             +-------+-------+
        |                             |
        |  (1) Publish Events         |
        |  (Topics: click-logs,       |
        |   orders, transactions)     |
        |                             |
        v                             v
+===================================================+
|               KAFKA CLUSTER                       |
|  +-------------+  +-------------+  +-----------+  |
|  | Broker 1    |  | Broker 2    |  | Broker 3  |  | <--- (2) Durable
|  | [Part 1 L]  |  | [Part 1 R]  |  | [Part 2 R]|  |      Storage &
|  | [Part 2 R]  |  | [Part 2 L]  |  | [Part 1 R]|  |      Replication
|  +-------------+  +-------------+  +-----------+  |
|          ^               ^               ^        |
|          |               |               |        |
|      (Zookeeper / KRaft Controller Quorum)        |
+===================================================+
        |              |              |
        | (3) Consume  |              |
        |     Groups   |              |
        v              v              v
+-------+-------+  +---+---+  +-------+-------+
|  Real-time    |  | Fraud |  |  Data Ware-   |
|  Analytics    |  | Detect|  |  house (S3 /  |
|  Service      |  | Svc   |  |  Snowflake)   |
+-------+-------+  +---+---+  +-------+-------+
        |              |
        v              v
    +---+---+      +---+---+
    | Redis |      |  SQL  |
    +-------+      +-------+
```

