# Quick Revision: Sharding

This is the **"Nuclear Option" i.e final option of database scaling**

In a System Design interview, you should ***exhaust all other options (Caching, Read Replicas, Vertical Scaling) before suggesting Sharding, because it introduces massive complexity***

## Table of Contents

- [Quick Revision: Sharding](#quick-revision-sharding)
  - [The Core Concept of Divide and Conquer](#the-core-concept-of-divide-and-conquer)
    - [How it Works using a Sharding Key](#how-it-works-using-a-sharding-key)
  - [Sharding and the CAP Theorem](#sharding-and-the-cap-theorem)
  - [Trade-offs and complexity of Sharding](#trade-offs-and-complexity-of-sharding)
  - [Selecting the right key for sharding](#selecting-the-right-key-for-sharding)

## The Core Concept of Divide and Conquer

Sharding is splitting a single large database into *smaller, easier-to-manage parts* called **Shards**. All shards have the *same schema* (table structure), but they *hold distinct rows of data*

1. Vertical Scaling: Buying a bigger machine (More RAM/CPU)
	- Limitation: There is a hardware ceiling
2. Horizontal Scaling (**Sharding**): Buying more machines. 
	- Limitation: ***Application complexity***

**The Goal**: If you have 1 Billion users, you don't store them on one machine. You put users `1-1M` on Server A, users `1M-2M` on Server B, etc

### How it Works using a Sharding Key

The *most critical decision* in sharding is picking the **Sharding Key (Partition Key)**. This key determines which server holds a specific row

Strategy A: **Range Based Sharding**
- Split data based on ranges of values
- Logic: if (`ID < 100`) -> Shard 1, if (`ID < 200`) -> Shard 2, ...
- Example: Storing User Data by Alphabet (`A-M` on Server 1, `N-Z` on Server 2)
```
 [ Router / Proxy ]
           |
           | WHERE name="Alice"
           v
   (Range: A-M)           (Range: N-Z)
  +-------------+       +-------------+
  |   Shard 1   |       |   Shard 2   |
  |  (Alice)    |       |  (Zack)     |
  |  (Bob)      |       |  (Xi)       |
  +-------------+       +-------------+
```
- Pros: **Great for Range Queries** (e.g., "Get users created between Jan and Feb").
- Cons: **Hotspots**. If everyone is named "David", "Daniel", "Dylan"... Shard 1 explodes while Shard 2 is empty

Strategy B: **Hash Based Sharding (Key Based)**
- Use a hash function to *randomize* the distribution.
- Logic: `Shard_ID = Hash(User_ID) % Total_Shards`
- Example: `Hash(123) % 4 = 1` -> Shard 1
```
      [ Router ]
          |
          | User_ID = 105
          v
     Hash(105) % 3 = 0
          |
          v
   +-------------+    +-------------+    +-------------+
   |   Shard 0   |    |   Shard 1   |    |   Shard 2   |
   |  (ID: 105)  |    |             |    |             |
   +-------------+    +-------------+    +-------------+
```
- Pros: **Even Distribution** i.e *Prevents hotspots naturally*
- Cons: 
	- **Resharding is painful** (If you add a server, the modulo changes)
	- **Bad for Range Queries** (You must query ALL shards to find "users created today")

Strategy C: **Directory Based (Lookup Table)**
- A *separate database* tells you where the data lives
- Logic: `Query Lookup_DB -> Returns "Shard A"`. Then Query "Shard A"
- Pros: **Flexible**. You can *move data without changing algorithms*
- Cons: 
	- **Single Point of Failure (The Lookup DB)** 
	- **Added latency** (Two DB lookups)

|Strategy|Definition      |Pros |Cons                                              |Best For                              |
|--------|----------------|-----|--------------------------------------------------|--------------------------------------|
|No Sharding|Vertical Scale (Bigger Machine)|Simple. Full ACID transactions.|Hard limit on RAM/CPU.                            |Startups, Small/Mid Apps.             |
|Key/Hash Sharding|Hash(ID) % N    |Even distribution.|Resharding is hard. No Range queries.             |High Scale, Key-Value lookups.        |
|Range Sharding|A-Z, Year 2020-2022|Good for Range Scans.|Hotspots (Unbalanced load).                       |Time-series data (Logs, Chat History).|

## Sharding and the CAP Theorem

Sharding is essentially **Partitioning (`P`)**. You are acknowledging that network partitions exist. Now you must choose:

**CP (Consistency + Partition Tolerance)**:
- If a Shard goes down (Partition), you ***block all writes to it***
- Result: You get errors, but data is never out of sync
- Use Case: Banking, Inventory

**AP (Availability + Partition Tolerance)**:
- If a Shard goes down, you might ***write to a "temporary" node or return stale data***
- Result: System stays up, but you might lose that write or see old data
- Use Case: Facebook Likes, YouTube Views

## Trade-offs and complexity of Sharding

3 main problems:
1. **The "Celebrity Problem" (Hot Partition)**: Even with Hash Sharding, if one user (e.g., Justin Bieber) gets 1000x more comments than anyone else, the Shard holding his data will die
	- *Fix*: ***Add "Salt" or random numbers*** to the key to split even his data across multiple shards

**Salt fix explanation**

*Explanation*: By appending a random suffix (like `_1`, `_2`) to the popular ID, you effectively break one massive "hot" key into distinct sub-keys that hash to different servers, spreading the load evenly instead of crushing a single node

```
      WITHOUT SALT (Hotspot)                  WITH SALT (Distributed)

 "Bieber" (1M reqs) --> [ Shard A ] ðŸ”¥       "Bieber_1" --> [ Shard A ] (330k)
                        [ Shard B ]          "Bieber_2" --> [ Shard B ] (330k)
                        [ Shard C ]          "Bieber_3" --> [ Shard C ] (330k)
```

**Salt fix flaw or drawback**

The major drawback is **Read Complexity (The "Scatter-Gather" Penalty)**

While splitting the key makes writing fast (distributed), it **makes reading slow**. To get the "total" data (e.g., total comments for Justin Bieber), you can no longer just query one shard. You must query every single split and aggregate the results.

The Trade-off
- Writes (Fast): `Bieber_1` goes to Shard A, `Bieber_2` goes to Shard B. (***Load Balanced***)
- Reads (Slow): To show his profile, the app must ask Shard A, Shard B, **AND** Shard C, wait for all of them, and sum the results

```
[ Application ]
             |
             | "Get Total Comments"
             | (Must query ALL splits)
             v
     +-------+-------+
     |       |       |
  (Read)  (Read)  (Read)
     |       |       |
     v       v       v
 [Shard A] [Shard B] [Shard C]
 (Count:5) (Count:3) (Count:2)
     |       |       |
     +-------+-------+
             |
             v
      [ Application ]
      (Sum = 5+3+2 = 10)
```

2. **Cross-Shard Joins (The Nightmare)**: If User is on Shard 1 and Orders are on Shard 2, you cannot run `SELECT * FROM User JOIN Orders`
	- Fix:
		- *Denormalization*: Duplicate User data into the Order table
		- *App-Level Join*: Fetch User, then fetch Orders, combine in code (***Slow***)

3. **Resharding (Data Rebalancing)**: If Shard A fills up, you need to split it into A1 and A2
	- Problem: Moving millions of rows while the database is live is incredibly *risky* and *CPU intensive*
	- Fix: **Use Consistent Hashing (discussed earlier) to minimize data movement**

## Selecting the right key for sharding

To select a sharding key, look at your **Access Pattern** (your most frequent queries). The "Right" key *minimizes cross-shard queries*

**The Selection Rule**
1. **Identify the Core Entity**: What ID is present in 90% of your `WHERE` clauses? (e.g., `user_id` for Facebook, `video_id` for YouTube, `chat_id` for Discord)
2. **Check Cardinality**: Does this key have *enough unique values to spread evenly*? (e.g., `Country_Code` is bad because "US" will be huge; `UUID` is good)

**How to Defend Your Choice (The Trade-off)**

Scenario A: "I choose Sharding by User_ID (Hashing)"
- Defense: "I chose this because our read queries are almost always for a specific user. This guarantees Uniform Distribution (no server is overloaded) and ensures 99% of queries hit just one shard."
- The Concession: "The trade-off is that analyzing 'Global Trends' requires querying all shards (Scatter-Gather), but that is a rare background job."

Scenario B: "I choose Sharding by Chat_ID (Locality)"
- Defense: "I chose this because in a chat app, users fetch 50 messages at a time. By putting a whole chat on one shard, we get Data Locality (one fast disk seek). If we sharded by Message_ID, we'd have to ping 50 different servers for one conversation."
- The Concession: "The trade-off is a potential Hot Partition if one chat group gets 1M users, which I would handle by splitting just that specific group."
