# Quick Revision: Data Modeling

Data modeling is arguably the most critical part of a System Design interview. If you pick the wrong database or schema, your system will either fail to scale or fail to remain consistent.

The modern approach (taught by Alex Xu/HelloInterview) flips the old school logic: **Don't start with the Schema. Start with the Access Patterns**

## Table of Contents

- [Quick Revision: Data Modeling](#quick-revision-data-modeling)
  - [Deep dive on keys](#deep-dive-on-keys)
  - [Visualizing various database models](#visualizing-various-database-models)

**1. The Menu: Database Options**

You must choose the right tool for the job.

*A. Relational Databases (SQL)*
- Examples: PostgreSQL, MySQL
- Structure: Tables, Rows, Columns. Rigid Schema
- Superpower: **ACID** Transactions & Joins. You can strictly relate `Users` to `Orders` and guarantee money isn't lost
- Best For: Financial systems, billing, strict inventory, structured data where relationships are complex.
- CAP Fit: Generally CP (**Consistency** favored) or CA (Single node)
```
  [ User Table ]        [ Order Table ]
  +----+-------+       +----+---------+
  | ID | Name  | <---> | ID | User_ID |
  +----+-------+       +----+---------+
     (Joined by Foreign Key)
```

*B. Document Databases (NoSQL)*
- Examples: MongoDB, DynamoDB (Document mode)
- Structure: JSON-like Documents. Flexible Schema
- Superpower: F**lexibility & Read Speed**. You store all related data together (*Denormalized*), so you read it in one go
- Best For: Product Catalogs, Content Management, User Profiles
- CAP Fit: Usually AP (**Availability** favored)
```
  {
     "id": 101,
     "name": "Alice",
     "orders": [          <-- Embedded Data
        { "id": "A1", "total": 50 },
        { "id": "A2", "total": 20 }
     ]
  }
```

*C. Key-Value Stores*
- Examples: Redis, DynamoDB (KV mode), Cassandra
- Structure: Dictionary (`Key -> Blob`)
- Superpower: **Speed & Scale. `O(1)` reads/writes**
- Best For: Caching, Sessions, Shopping Carts, Counters
- CAP Fit: **AP (Dynamo/Cassandra)** or **CP (Redis depending on config)**

*D. Graph Databases*
- Examples: Neo4j
- Structure: Nodes and Edges
- Superpower: **Complex Relationships**. "Find friends of friends who like Jazz"
- Best For: Social Networks, Recommendation Engines, Fraud Detection

**2. The Design Process: Step-by-Step**

Do not just draw an ER Diagram. Follow this flow:

*Step 1: Requirements & Access Patterns (The "What")*
- Before defining a table, ask: "How will we read this?"
- Write Pattern: "Users create posts (1k/sec)."
- Read Pattern: "Users view their own feed (100k/sec)." -> Read Heavy

*Step 2: Entities & Relationships*
- Identify the nouns: User, Post, Follow
- 1:1 (User -> Profile)
- 1:Many (User -> Posts)
- Many:Many (User <-> User [Followers])

*Step 3: Normalization vs. Denormalization (The Trade-off)*
- Normalization (SQL Style):
	- Concept: Store data once. Use Joins to connect them
	- Pros: **Write fast (update 1 row). Consistency is easy**
	- Cons: **Reads are slow (Joins are expensive)**
- Denormalization (NoSQL Style):
	- Concept: Duplicate data to optimize for reads. Store `User_Name` inside the Post document
	- Pros: **Reads are instant (no joins)**
	- Cons: **Writes are complex** (If user changes name, you must update 1,000 posts). **"Write Amplification"**

```
    NORMALIZED (SQL)                  DENORMALIZED (NoSQL)
    
    [Post Table]                      [Post Document]
    | ID | User_ID | Content |        {
    | 10 | 500     | "Hello" |           "id": 10,
                                         "content": "Hello",
    [User Table]                         "user": {
    | ID  | Name  |                             "id": 500,
    | 500 | Alice |                             "name": "Alice"  <-- Duplicated!
                                         }
                                      }
```

**3. Keys & Indexing**

*Primary Keys*
- **UUID**: Unique, but large (128-bit). Random (*bad for B-Tree locality*)
- **Auto-Increment (1, 2, 3)**: Small, fast, but reveals growth info (*security risk)*
- **Snowflake ID (Twitter)**: Time-sortable, unique, numeric (*Best for System Design)*

*Indexing*
- An index is a *data structure (B-Tree)* that sorts a column **for fast lookup**
- Without Index: Scan 1 Million rows (`O(N)`)
- With Index: Binary Search (`O(log N)`)
- Trade-off: **Indexes make Reads Fast but Writes Slow (you must update the index on every insert)**

**4. Scaling: Sharding & The CAP Theorem**

When the *data gets too big for one node*, you must **Shard** (Partition). This decision ties directly to CAP

*The CAP Connection*:
- SQL Sharding (Manual):
	- You split users by ID
	- CAP: You are usually moving *towards CP*. If a shard is down, you fail the write to ensure consistency
- NoSQL Sharding (Auto - e.g., DynamoDB/Cassandra):
	- You define a Partition Key (e.g., User_ID)
	- The DB automatically distributes data
	- CAP: *Usually AP*. If a partition is unreachable, the system accepts the write on a replica and reconciles later ("Eventual Consistency")

Selecting a Sharding Key
- Bad Key: `Timestamp` (Everyone writes to the "Today" shard -> **Hotspot**)
- Good Key: `User_ID` (Evenly distributed)

```
      [ Load Balancer ]
             |
             v
     (Sharding Strategy: User_ID % 3)
      /      |      \
  [Shard A] [Shard B] [Shard C]
  (Users 1,4) (Users 2,5) (Users 3,6)
```

|Feature|Relational (SQL)|Document (NoSQL)|Key-Value                                                                                                                                                                 |Graph               |
|-------|----------------|----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------|
|Schema |Rigid           |Flexible        |None                                                                                                                                                                      |Nodes/Edges         |
|Joins  |Yes (Strong)    |No (Embed data) |No                                                                                                                                                                        |Yes (Graph traverse)|
|Transactions|ACID (Strong)   |Base/Local ACID |Atomic (Single Key)                                                                                                                                                       |ACID (varies)       |
|Normalization|Normalized      |Denormalized    |Denormalized                                                                                                                                                              |Normalized          |
|Best For|Finance, ERP, Structured data|Content, Catalogs, User Data|Caching, Counters                                                                                                                                                         |Social, Fraud       |
|Scale  |Vertical (easiest), Horizontal (hard)|Horizontal (native)|Horizontal (native)                                                                                                                                                       |Vertical (mostly)   |

## Deep dive on keys

Choosing a **Primary Key** is about balancing Speed (Database performance), Scale (Distributed systems), and Security

**1. Auto-Increment (1, 2, 3...)**
- The Concept: A simple counter managed by the database. The first user is 1, the second is 2
- The Good (Speed):
	- Smallest Size: Itâ€™s just a 64-bit integer. Very efficient to store
	- **Perfect for B-Trees**: Databases store data in sorted trees. Adding 4 after 3 is fast because it goes right at the end. It keeps the "pages" of the book filled neatly in order.
- The Bad (Scale & Security):
	- **Sharding Nightmare**: If you split your database into two servers, both might try to create User 105 at the same time. You get a conflict
	- Security Risk: If a user signs up and sees their ID is 500, they know you only have 500 users. If they sign up tomorrow and get 505, they know you grew by 5 users. Predictability.

**2. UUID (Universally Unique Identifier)**
- The Concept: ***A massive, random 128-bit string*** generated by the application, not the database (e.g., 550e8400-e29b...)
- The Good (Scale):
	- **Collision Free**: You can generate these on 1,000 different servers simultaneously without talking to each other, and they will never clash
	- **Security**: Completely random. Reveals nothing about your growth or order.
- The Bad (Performance):
	- **Huge**: Takes up 4x more space than an Integer
	- **Database Killer (Bad Locality)**: Because UUIDs are random, the database cannot just "append" the new row to the end. ***It has to jump around the hard disk to find the "right spot" in the B-Tree to insert A between 9 and B***. This causes ***fragmentation*** and ***makes inserts much slower as data grows***

**3. Snowflake ID (The "Goldilocks" Solution)**
- The Concept: ***A hybrid 64-bit integer invented by Twitter***. It is composed of: `[Timestamp] + [Machine ID] + [Sequence Number]`
- The Good (Best of Both Worlds):
	- **Time-Sortable**: Since the first part is a timestamp, `ID 200` is guaranteed to be *older than* `ID 300`. ***Your database can append it to the end (Fast like Auto-Increment)***
	- **Distributed**: It includes the "Machine ID", so Server A and Server B can generate IDs at the same time ***without conflicts (Scalable like UUID)***
	- Small: It fits into a standard 64-bit integer (***half the size of a UUID***)
- The Bad:
	- **Complexity**: You need ***a dedicated service or logic to generate them***; the *database doesn't do it by default*

|Feature|Auto-Increment  |UUID (v4)|Snowflake ID                                                                                                                                                              |
|-------|----------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|Size   |Tiny (64-bit)   |Huge (128-bit)|Tiny (64-bit)                                                                                                                                                             |
|Database Write Speed|Fastest (Sequential)|Slow (Random jumps)|Fast (Mostly Sequential)                                                                                                                                                  |
|Distributed / Sharding|Impossible (Conflicts)|Excellent|Excellent                                                                                                                                                                 |
|Security|Poor (Leaks info)|Best (Random)|Good (Obscure)                                                                                                                                                            |
|Verdict|Use for small/internal apps.|Use if write speed < scale.|Use for High Scale Systems.                                                                                                                                               |

## Visualizing various database models

**1. Relational (SQL)**
- Structure: Rigid tables with fixed columns and rows
```
+----+-------+-----+
| ID | Name  | Age |
+----+-------+-----+
| 1  | Alice | 25  |
| 2  | Bob   | 30  |
+----+-------+-----+
```
- Best For: Structured data, Financial systems (ACID transactions), Complex Joins
- Pros: Strong consistency, standardized query language (SQL)
- Cons: Hard to scale horizontally (requires sharding); Schema migrations are painful

**2. Key-Value Stores (NoSQL)**
- Structure: A giant Hash Map. `O(1)` lookups
```
Key          Value
-----------------------------
"user:1" ->  "Alice"
"sess:9" ->  "xyz_token_123"
```
- Best For: Caching (Redis), Session Management, Shopping Carts.
- Pros: Ultra-fast reads/writes, Simple model
- Cons: No queries (can only fetch by Key), No relationships between data

**3. Document Stores (NoSQL)**
- Structure: Flexible JSON-like documents. Schema-less
```
Key: "user:1"
Value: {
  "name": "Alice",
  "tags": ["engineer", "gamer"],  <-- Nested Data
  "meta": { "active": true }
}
```
- Best For: Content Management (CMS), Catalogs, User Profiles (variable attributes)
- Pros: Flexible schema (add fields anytime), handling semi-structured data
- Cons: ACID support is often limited; Joins are expensive or impossible

**4. Wide-Column (NoSQL)**
- Structure: Two-dimensional key-value store. Columns can vary per row and are grouped into "Families."
```
Row Key  |  Column Family: Personal  |  Column Family: Activity
---------------------------------------------------------------
User1    |  name:Alice, email:a@x.com|  last_login:10am
User2    |  name:Bob                 |  last_login:1pm, click:btn1
```
- Best For: High write throughput, Time-series data (IoT, Metrics), Big Data (Cassandra/HBase).
- Pros: Scales massively (Petabytes), optimized for writing
- Cons: Rigid access patterns (You must design the table based on exactly how you will query it); No Ad-hoc queries

**5. Graph Databases**
- Structure: Nodes (Entities) connected by Edges (Relationships).
```
(Alice) --[FRIEND_OF]--> (Bob)
   |
   +--[LIKES]--> (Pizza)
```
- Best For: Social Networks, Recommendation Engines, Fraud Detection
- Pros: Extremely fast for traversing deep relationships (Friend of a Friend)
- Cons: Difficult to scale (sharding a graph is hard mathematically)

|Type  |Example Tools   |Pick it if...|
|------|----------------|-------------|
|Relational|Postgres, MySQL |You need ACID or have fixed schema.|
|Key-Value|Redis, DynamoDB |You need speed and simple lookup.|
|Document|MongoDB         |You have complex, changing data structures.|
|Wide-Column|Cassandra       |You have massive writes (IoT/Logs).|
|Graph |Neo4j           |You need to find relationships.|
