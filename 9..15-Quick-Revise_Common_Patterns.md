# Quick Revision: Common Patterns

This is the "Toolbox" section of your interview. When the interviewer presents a problem (e.g., "Design a Ticket Booking System" or "Design WhatsApp"), you don't invent solutions from scratch. You pull one of these **Standard Patterns** out of your toolbox

## Table of Contents

- [Quick Revision: Common Patterns](#quick-revision-common-patterns)
  - [Pushing Realtime Updates](#pushing-realtime-updates)
  - [Managing Long-Running Tasks or Async Processing](#managing-long-running-tasks-or-async-processing)
  - [Dealing with Contention or The Ticketmaster Problem](#dealing-with-contention-or-the-ticketmaster-problem)
  - [Handling Large Blobs with The "Passthrough" Anti-Pattern](#handling-large-blobs-with-the-passthrough-anti-pattern)
  - [Multi-Step Processes in Distributed Transactions](#multi-step-processes-in-distributed-transactions)
  - [Scaling Reads vs Scaling Writes](#scaling-reads-vs-scaling-writes)
  - [Proximity-Based Services for Location](#proximity-based-services-for-location)
    - [GeoHashing solution](#geohashing-solution)
    - [Quadtree Solution or The Dynamic Box Approach](#quadtree-solution-or-the-dynamic-box-approach)
    - [Which proximity algorithm to choose and when](#which-proximity-algorithm-to-choose-and-when)
  - [Summary strategy for selecting a pattern](#summary-strategy-for-selecting-a-pattern)

## Pushing Realtime Updates

Scenario: You need to notify a user immediately when something happens (e.g., New Message, Stock Price Update, Uber Driver Arrived)

**Option A: Short Polling (The Naive Approach)**
- The client asks the server "Any new data?" every 2 seconds
- Pros: **Simplest** to build. **Standard HTTP**
- Cons: **Wasteful**. 99% of checks return "No." **Kills mobile battery** and **server CPU**
- Use Case: ***Dashboard stats where 5-second delay is fine***

**Option B: Long Polling (Hanging GET)**
- The client asks "Any new data?". The ***server does not reply until data arrives (or timeout)***
- Pros: **Real-time feel**. Works over **standard HTTP**
- Cons: **Server keeps a connection open (expensive RAM)**. **Header overhead on reconnect**
- Use Case: **Notification bells**, **simple chat apps (early WhatsApp)**

**Option C: WebSockets (The Gold Standard)**
- A ***persistent, bi-directional pipe*** between Client and Server
- Pros: **True Real-time**. **Extremely low latency**. Can send data *both* ways (Chat)
- Cons: **Complex to scale (Stateful)**. Load balancers need "Sticky Sessions"
- Use Case: **WhatsApp**, Multiplayer Games, **Collaborative Editing** (Google Docs)

**Option D: Server-Sent Events (SSE)**
- A **one-way megaphone**. Server shouts to Client. Client cannot shout back
- Pros: **Simpler than WebSockets (uses standard HTTP)**. **Auto-reconnects**
- Cons: **One-way only**
- Use Case: **Stock Tickers, Live Sports Scores, News Feeds**

```
        [ Polling ]                   [ WebSocket ]
    Client      Server            Client       Server
      | -- Ask? -> |                 | -- Open --> |
      | <- No ---- |                 | <--- OK --- |
      | (Wait 2s)  |                 |             |
      | -- Ask? -> |                 |             |
      | <- No ---- |                 | <-- DATA -- | (Push!)
      | (Wait 2s)  |                 | <-- DATA -- | (Push!)
      | -- Ask? -> |                 | -- DATA --> | (Send!)
      | <- YES --- |                 |             |
```

## Managing Long-Running Tasks or Async Processing

Scenario: A user uploads a video to YouTube. Processing it takes 10 minutes. HTTP requests timeout after 30 seconds. You cannot make the user wait

The Pattern: **Queue-Based Decoupling**
1. **Accept**: API receives request, returns **`202` Accepted** *immediately*
2. **Queue**: API pushes a "Job" to a **Message Queue** (***SQS/Kafka***)
3. **Process**: A **background "Worker" picks up the job** and crunches it
4. **Notify**: Worker updates the status in DB or **sends a webhook/notification**

**CAP Tie-in**: This moves the system from CP (Waiting for consistency) to AP (**Available immediately, consistent eventually**)

```
    [ User ]
       | (1) Upload Video
       v
    [ API Server ] -- (2) Return "202 ID:55" -> [ User ]
       |
       | (3) Push Job
       v
    +----------+         (4) Pull Job        +----------+
    | Msg Queue| --------------------------> |  Worker  |
    +----------+                             +----------+
                                                  | (5) Process
                                                  | & Update DB
```

## Dealing with Contention or The Ticketmaster Problem

Scenario: 10,000 users try to buy the last concert ticket at the exact same millisecond

**Option A: Pessimistic Locking (The Brute Force)**
- "**Lock the row**." When User A reads the ticket, the DB locks it. User B waits until A finishes
- Pros: **Guarantees safety (CP).** No double bookings
- Cons: **Slow**. **Blocks** everyone. Can cause **Deadlocks**
```
Time   User A                  Database (Row: Ticket #99)     User B
 |        |                           |                          |
 1        |-- SELECT FOR UPDATE ----->| [ LOCK ACQUIRED ]        |
          |                           |                          |
 2        |      (Thinking...)        |<-------------------------| (Try to Read)
          |                           |          ||              |
 3        |-- BUY TICKET ------------>|        BLOCK!            |
          |                           |          ||              |
 4        |-- COMMIT ---------------->| [ LOCK RELEASED ]        |
          |                           |                          |
 5        |                           |<-------------------------| (Finally Reads)
          |                           | "Sold Out"               |
```

**Option B: Optimistic Locking (The "Version" Check)**
- Add a **version column to the row**
- User A reads Ticket (v1). User B reads Ticket (v1)
- User A updates: `SET user=A, version=2 WHERE id=1 AND version=1`. Success *(Now ticket version is v2)*
- User B updates: `SET user=B, version=2 WHERE id=1 AND version=1`. FAIL. (`Rows modified = 0`)
	- User B sees failure and retries!
- Pros: **Fast**. No database locks held.
- Cons: **High failure rate under heavy load**
```
User A (Fast)           Database (Ver: 1)           User B (Slow)
         |                           |                          |
         |<----- Read Ticket (v1) ---|--- Read Ticket (v1) ---->|
         |                           |                          |
   (Has v1)                                                (Has v1)
```
```
      User A                      Database                    User B
         |                           |                          |
         | (1) UPDATE ...            |                          |
         | WHERE ver=1               |                          |
         |-------------------------->| (Check: is Ver=1?)       |
         |                           | YES -> Update to v2      |
         | [ SUCCESS ]               |                          |
                                     |                          |
                                     | (2) UPDATE ...           |
                                     | WHERE ver=1              |
                                     |<-------------------------|
                                     | (Check: is Ver=1?)       |
                                     | NO! It is v2.            |
                                     | [ FAIL / 0 Rows ]        |
```

**Option C: Distributed Locking (Redis)**
- Use a ***shared Redis instance*** to **hold a "mutex" lock (`SETNX`)**
- Services ***check Redis before touching the DB***
- Use Case: When the ***resource isn't a single DB row*** (e.g., "Only allow 1 user to run the Daily Report")

Think of Redis as "The Bouncer"
```
                [ REDIS CLUSTER ]
               (Holds "Key: Ticket_99")
                  ^            ^
           (1)    |            | (2)
        SETNX OK  |            | SETNX FAIL (0)
                  |            |
             [ APP A ]      [ APP B ]
                  |            |
            (3)   |            | (4)
        Write DB  |            | Return Error
                  v            v
            [  DATABASE  ]   (User sees "Busy")
```

## Handling Large Blobs with The "Passthrough" Anti-Pattern

Scenario: User uploads a 5GB file. 
- **Bad Pattern**: `User -> API Server -> S3`
- Why: You *burn expensive API CPU/RAM handling raw bytes*. You block threads!

The Pattern: **Presigned URLs**
- Client asks API: "I want to upload `cat.mp4`"
- API talks to S3: "Generate a secure, temporary URL for this user."
- API gives URL to Client
- Client uploads directly to S3

```
 1. Request Upload      2. Return URL
    [ Client ] <---------------------> [ API Server ]
        |                                     ^
        |                                     | (Auth check)
        | 3. PUT file directly                |
        v                                  [ AWS S3 ]
    [ Object Storage ] <----------------------+
```

## Multi-Step Processes in Distributed Transactions

Scenario: "Book Trip" involves: 
1. Charge Credit Card
2. Book Flight
3. Book Hotel
4. If #3 fails, you must undo #1 and #2.

The Pattern: The **Saga Pattern** Since we can't have one giant ACID transaction across 3 different microservices, we use a *chain of local transactions*

Saga Pattern can be of two types:
1. Choreography (Event-based)
2. Orchestration (Coordinator-based)

**Choreography (Event-Based)**:
3. Payment Service finishes -> Emits `PaymentSuccess`
4. Flight Service hears `PaymentSuccess` -> Books Flight.
5. Rollback: If Flight fails, it emits `FlightFailed`. Payment Service hears this and runs `Refund`

Best for: *Simple, linear flows* (e.g., `User Signup -> Send Welcome Email`)

```
[Payment Svc]
     |
     +---(Event: Paid)----> [Flight Svc]
                                  |
                                  +---(Event: Booked)----> [Hotel Svc]
                                                                |
                                             (If Fail) <--[Event: Error]
```

**Orchestration (Conductor)**: A central "Coordinator" service tells each service what to do and handles errors
- Orchestration is *tighter coupling* (coordinator knows everyone), but *simpler to manage business logic changes*

Best for: *Complex Workflows* (e.g., Book Trip, Order Fulfillment). When you have *critical financial rollbacks* (Compensating Transactions) involving 3+ services, you need a central coordinator to ensure the refund happens if the hotel booking fails. *Choreography is too risky for complex error handling here*

```
           +------------------+
           |   TRIP MANAGER   | (The "Brain")
           +--------+---------+
          /         |          \
   (1) Charge?  (2) Book?    (3) Reserve?
      /             |              \
     v              v               v
[Payment Svc]  [Flight Svc]   [Hotel Svc]
     ^              ^               |
     |              |               |
     \______________|_______________/
      (4) If Hotel fails, Manager explicitly
          calls "Refund" on Payment Svc.
```

Use Case: **Uber** (`Ride Request -> Lock Driver -> Charge Card`)

## Scaling Reads vs Scaling Writes

**Scaling Reads (The "Fan Out")**
- Goal: Serve 1M users reading the same news article
- **Pattern: `Caching (CDN + Server)` + `Read Replicas`**

Example:
- Add 5 Read Replicas (Slaves) to your Database
- Put a CDN in front for images.
- Put Redis in front for data

*CAP Theorem*: Moves towards AP (**Availability**). Replicas might lag behind the master by a few milliseconds

```
                   [  Users (1M)  ]
                        ||||| (Massive Read Traffic)
                        vvvvv
          /-------------+----------------\
         v                                v
[ CDN (Images/CSS) ] <-----> [ App Load Balancer ]
         |                                |
    (Hits return fast)                    v
                                  [ App Servers ]
                                          |
                       /------------------+------------------\
                      v (Read Path)                           v (Write Path)
             [ Redis Cache ]                           [ Primary DB ]
              | (Cache Miss)                                  |
              v                                               |
    [ DB Read Replicas (x5) ] <----(Async Sync)---------------|
```

**Scaling Writes (The "Fan In" / Sharding)**
- Goal: Ingest 1M sensor logs per second
- **Pattern 1: Message Queue Buffer**
	- Write to *Kafka* first (sequential IO, super fast)
	- Workers drain Kafka to DB at a *steady pace*
	- Best Use Case: **Spiky Traffic**. (e.g., Black Friday sales, 1M IoT sensors bursting data at once).
	- Trade-off: Data is not queryable instantly (**Eventual Consistency**)
- **Pattern 2: Sharding**
	- Split the database into 10 pieces
	- Best Use Case: **Infinite Growth**. (e.g., Storing all of Facebook's user data)
	- Trade-off: Operational nightmare (**Joins across shards are impossible)**

```
   (Massive Spike!)
      ||||||||
      vvvvvvvv
   [  KAFKA   ]  <-- Writes are O(1) (Append only)
   (Holds Data)
        |
     (Steady Stream)
        |
        v
   [  WORKER  ]  <-- Process at safe speed
        |
        v
   [ DATABASE ]  <-- Survives!
```

```
    (High Load)
       |||||
       vvvvv
   [  ROUTER  ]  (Hash: UserID % 2)
      /     \
     /       \
    v         v
[ DB 1 ]    [ DB 2 ]
(Users A-M) (Users N-Z)
```

## Proximity-Based Services for Location

Scenario: "Find drivers near me." 

- Naive Way: Calculate distance to all drivers. (***Too slow***)
- The Pattern: **Geohashing / QuadTrees Divide the map into a grid**
	- Geohash: `"New York" = dr5r`, `"Times Square" = dr5ru`

Storage: 
- Store drivers in Redis: `Key: dr5ru`, `Value: [DriverA, DriverB]`
- Search: User is in `dr5ru`. Just query that one key

### GeoHashing solution

Geohashing is also called the **"Grid String"** Approach

Concept: 
- ***Divide the world into fixed square grids***
- Every ***grid has a unique alphanumeric string***
- ***The longer the string, the smaller and more precise the square***

Step-by-Step Flow:
1. Driver Update: Driver A sends `GPS (40.7, -74.0)` (i.e latitude & longitude). System ***converts the GPS lat & long value*** this to a **Geohash** `"dr5ru"`
2. Storage: Save to Redis (A fast in-memory store)
3. User Search: User is at `(40.7, -74.0)`. System converts to `"dr5ru"`
4. Expansion: ***System calculates the 8 neighboring grids*** (e.g., `dr5rt`, `dr5rv`, ...) and queries those keys too!

Database Storage (Redis/Key-Value): We use the *Geohash* as the *Key*
```
Key (Grid ID)      Value (List of Drivers)
------------------------------------------
"dr5ru"       ->   [ "Driver_A", "Driver_B" ]
"dr5rt"       ->   [ "Driver_C" ]
```
Notice how "dr5ru" is inside "dr5r". `Shared prefixes = Proximity`
```
World ("d")
           |
      Region ("dr")
           |
    +------+------+
    | dr5r | dr5s |
    +------+------+
        |
    +---+---+
    |dr5ru| ...
    +---+---+
      ^
      |
 User & Driver A are here!
```

**Q: How do we fetch neighboring grids?**
- We calculate them mathematically before we query the database
- Since the Earth is a fixed grid, if you are in box `dr5ru`, we know the ***exact latitude/longitude boundaries of that box***
- The algorithm calculates the coordinates of the 8 surrounding boxes (N, S, E, W, NE, NW, SE, SW) and converts them into their geohash strings
- The Workflow:
	- Input: User is in `dr5ru`
	- Algorithm: Math library calculates `neighbors -> ["dr5ru", "dr5rt", "dr5rv", "dr5rs" ...]` (9 keys total)
	- Query: Send one fast batch request to Redis: `MGET(dr5ru, dr5rt, dr5rv...)`

**Q: Why do we use Redis (In-Memory)?**
- Two reasons: **Speed** and **Volatility**
- ***Extreme Write Load***: Drivers send GPS updates *every 3 seconds*
- A traditional disk-based database (Postgres/MySQL) would crumble under millions of updates per second
- RAM (Redis) handles this effortlessly
- ***Data is Ephemeral***: We don't care about where a driver was 10 seconds ago. Data "expires" instantly. We don't need the heavy durability/safety guarantees of a SQL database

### Quadtree Solution or The Dynamic Box Approach

Concept: 
- ***Start with the whole world*** (Unlike GeoHashing where it is already split into fixed square grids)
- ***If a box has "too many" drivers (e.g., > 100), split it into 4 smaller quadrants. Repeat recursively***

**Adding a driver**

Step-by-Step Flow:
1. Driver Update 
	- Logic: Inserting a driver isn't just saving a key; it's a ***Graph Traversal***. We start at the **Root** (The entire Map) and ask a series of recursive questions to "drill down" to the correct local bucket
	- Step A: Start at Root (Global Map)
	- Step B: Check Driver's `GPS: (Lat: 40, Lon: -70)`
	- Step C: Ask: "Does this belong in the North-West, North-East, South-West, or South-East quadrant?"
	- Step D: It belongs in North-East. Move to that node
	- Step E: Repeat until we hit a Leaf Node (a node with no children)
	- Step F: Add Driver ID to that Leaf Node's list

```
      [ ROOT (World) ]
             |
      (Is Lat > 0 and Lon > 0?) -> YES (Go NE)
             |
             v
        [ NE Node ]
             |
      (Is Lat < 45?) -> YES (Go SE)
             |
             v
      [ NE-SE Node ]  <-- Driver A stored here!
```

Database Storage (Document/Object Store): Quadtrees are often built *In-Memory* for speed. If persisted, we store the Node structure.
```
Node_ID   Boundary_Box           Children_IDs       Drivers
-----------------------------------------------------------
"Root"    [-90,90, -180,180]     [NW, NE, SW, SE]   []
"NE"      [0,90, 0,180]          [NE1, NE2...]      []
"NE1"     (Specific NYC Box)     null               ["Driver_A"]
```
- Dense areas (City) get more splits
- Sparse areas (Ocean) stay as big boxes
```
        [ ROOT ]
       /  |  |  \
     NW   NE SW  SE
          |
     (Too crowded!)
      / | | \
    1   2 3  4
        |
   (Driver A is here)
```

**Split (The "Cell Division")**

Logic: This is how Quadtrees handle ***Density***. A static grid (Geohash) has the same resolution everywhere. ***A Quadtree adapts***. If a specific city block gets too popular (e.g., a concert ends), that ***specific block subdivides to manage the load***.
- The Trigger: A node reaches its limit (e.g., Max_Capacity = 4)
- The Action:
	- Create 4 new empty child nodes (NW, NE, SW, SE) linked to the current node
	- Redistribute: Take the 4 existing drivers + the 1 new driver (5 total)
	- Re-evaluate their positions and move them down into the correct new child nodes
	- The current node is cleared of data and becomes a "Router" (Internal Node)
```
BEFORE (Leaf Node Full)           AFTER (Split into 4)
+-----------------------+        +-----------+-----------+
| Driver1   Driver2     |        | [NW]      | [NE]      |
|                       |        |  Driver1  |  Driver3  |
| Driver3   Driver4     |   ->   |  Driver2  |           |
|                       |        +-----------+-----------+
| (New Driver 5 arrives)|        | [SW]      | [SE]      |
+-----------------------+        |  Driver4  |  Driver5  |
                                 +-----------+-----------+
```

**User Search (The "Radius" Challenge)**

Logic: Finding the user's node is easy. ***The hard part is finding nearby drivers who might be in a different node***.
- Step A: Traverse the tree to find the Leaf Node containing the User. (e.g., User is in NE-SE)
- Step B: Return all drivers in that node
- The Problem: ***What if the user is standing on the edge of the box? The closest driver might be 1 meter away, but across the invisible line in the NE-SW box***
- The Fix: We don't just query the user's node. ***We query the Parent's other children (siblings)*** or ***draw a circle and query all nodes that intersect the circle***

```
Node A (User Here)      Node B (Driver Here)
+-----------------------||-----------------------+
|                       ||                       |
|          User         ||      Driver           |
|           o           ||         x             |
|                       ||                       |
+-----------------------||-----------------------+
           ^
           |
      The "Border"
(If we only check Node A, we miss the closest driver!)
```

### Which proximity algorithm to choose and when

QuadTree solution is more *dynamic* and *adaptive* compared to GeoHashing. ***Why do we not use QuadTree in all cases?***

1. **Rebalancing is Heavy**: When a driver moves from Square A to Square B, you have to remove them from one node and add them to another. In a QuadTree, if a node becomes empty or too full, the tree might need to merge or split (rebalance).
2. **Locking**: Modifying a tree structure usually requires locking parent nodes. If 10,000 drivers move at once, the tree locks up
3. **Geohashing is just a String Update**: Updating a Geohash is just `UPDATE drivers SET hash="abc"`. It is *instant* and doesn't change the underlying grid structure

|Feature|Geohash (Uber)  |Quadtree (Yelp/Google Maps)   |
|-------|----------------|------------------------------|
|Grid Size|Fixed. (A grid in NYC is same size as a grid in the Sahara).|Dynamic. (Small grids for cities, giant grids for oceans).|
|Implementation|Simple. Standard string libraries available.|**Complex. Requires tree re-balancing logic**|
|Updates|Fast. Just update a `Key-Value` pair.|**Harder. Moving a driver might trigger a tree split/merge**|
|Verdict|Use for **Real-time Moving Objects** (Taxi/Food Delivery).|Use for Static **POIs (Restaurants, Gas Stations)**|

## Summary strategy for selecting a pattern

|Question|Condition (Yes/No)|Choice (Pattern)              |
|--------|------------------|------------------------------|
|Is it heavy media (Video/Images)?|Yes               | **Presigned URLs (Direct to S3)** |
|Is it slow computation?|Yes               |**Async Workers (Message Queues)**|
|Is it a complex multi-step flow?|Yes               |**Saga Pattern (State Machine)**  |
|Is it Real-time (Bi-directional)?|Yes (e.g., Chat)  | **WebSockets**               |
|Is it Real-time (One-way)?|Yes (e.g., Stock Ticker)| **Server-Sent Events (SSE)**      |
|Is it High Contention?|Yes (e.g., 1 Ticket left)| **Distributed Lock / Optimistic Locking** |
|Need massive Read Scale?|Yes               | **Cache + Read Replicas**         |

