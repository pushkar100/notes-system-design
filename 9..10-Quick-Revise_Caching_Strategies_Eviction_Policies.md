# Caching strategies and eviction policies

Caching is one of the highest ROI topics in system design. It is the ***primary mechanism to trade Storage (RAM) for Speed (Latency) and Scale (Throughput)***

We need to identify two things about a cache while implementing it:
1. **Cache strategy** (3 types)
2. **Cache eviction policy** (4 types)

## Table of Contents

- [Caching strategies and eviction policies](#caching-strategies-and-eviction-policies)
  - [The Core Concept is Locality of Reference](#the-core-concept-is-locality-of-reference)
    - [The Layers or Where can we cache?](#the-layers-or-where-can-we-cache)
  - [Caching Strategies based on Reading and Writing](#caching-strategies-based-on-reading-and-writing)
    - [Cache-Aside or Lazy Loading](#cache-aside-or-lazy-loading)
    - [Read-Through or Write-Through](#read-through-or-write-through)
    - [Write-Behind or Write-Back](#write-behind-or-write-back)
  - [Cache eviction policies for the right cleanup](#cache-eviction-policies-for-the-right-cleanup)
    - [Least Recently Used or LRU](#least-recently-used-or-lru)
    - [Least Frequently Used or LFU](#least-frequently-used-or-lfu)
    - [First In First Out or FIFO](#first-in-first-out-or-fifo)
    - [Time To Live or TTL](#time-to-live-or-ttl)
    - [Comparison of eviction policies](#comparison-of-eviction-policies)
  - [Examples for cache strategies or policies](#examples-for-cache-strategies-or-policies)
  - [How the Thundering Herd problem affects caching](#how-the-thundering-herd-problem-affects-caching)

## The Core Concept is Locality of Reference

Caching works because real-world access patterns are *not random*.

- ***Temporal Locality***: If you requested data now, you are likely to request it again soon (e.g., refreshing a news feed)
- ***Spatial Locality***: If you requested data X, you are likely to request data near X (e.g., database rows stored physically together)

**The Goal**: ***Store frequently accessed data in fast, expensive memory (RAM) to avoid slow, expensive trips to the disk (Database)***

### The Layers or Where can we cache?

Cache can happen in every loop. However, focus primarily on **Distributed Cache**, but **mention CDN** for static media during system design interviews

```
  [User Browser]  <-- 1. Browser Cache (Images/CSS)
        |
        v
      [CDN]       <-- 2. Content Delivery Network (Static Assets @ Edge)
        |
        v
 [Load Balancer]  <-- 3. Reverse Proxy Cache (Varnish/Nginx)
        |
        v
  [App Server]    <-- 4. Local Cache (In-Memory HashMap)
        |
        v
 [GLOBAL CACHE]   <-- 5. DISTRIBUTED CACHE (Redis/Memcached) **(!Most Important!)**
        |
        v
   [Database]     <-- 6. DB Buffer Pool (Internal DB RAM)
```

## Caching Strategies based on Reading and Writing

You must choose the right strategy based on the Consistency vs. Latency (performance) trade-off

Strategies:
1. Cache-Aside (Lazy Loading)
2. Read-Through / Write-Through
3. Write-Behind (Write-Back)

### Cache-Aside or Lazy Loading

Cache-Aside (Lazy Loading). Usually the **Default Choice**
- The application is responsible for orchestrating the flow
- The *cache doesn't know about the DB*

Example flow:
- **App asks Cache**: "Do you have Key X?"
- Cache hit: Return X
- Cache miss: **App reads X from DB**
- **App writes X to Cache**
- Return X
```
 (1) Get Key X
      +-----------> [ Cache ]
      |                |
      | (2) Miss!      |
      | <--------------+
 [ Application ]
      |
      | (3) Read Data
      +-----------> [ Database ]
      |                |
      | (4) Return X   |
      | <--------------+
      |
      | (5) Write X
      +-----------> [ Cache ]
```

- **Pros**: ***Resilient*** (If cache fails, system still works via DB), ***Data Model*** (Cache data can be different structure than DB).
- **Cons**: ***Stale Data*** (DB updates don't automatically update Cache), ***Latency*** (3 network trips on a Miss).

Use Case: **General purpose Read-Heavy applications (Social Media, News)**. Reasons:
- **Memory Efficiency (The "Viral" Filter)**: Social media has millions of old, unpopular posts. With Cache-Aside, *data is only cached **after** someone asks for it*. This naturally filters out "cold" data, ensuring expensive cache memory is used only for "hot/viral" content that people are actually reading.
- **Resilience (Fail-Safe)**: Since the *application knows how to talk to the Database directly (on a cache miss)*, if the Cache crashes, the website stays online (it just gets ***slower***).

### Read-Through or Write-Through

The Application treats the **Cache as the main data store**. The Cache *itself* connects to the DB

Write-Through Flow:
1. App writes to Cache
2. Cache writes to DB (Synchronously).
3. ***Both confirm success*** (!IMPORTANT!)

Read-Through Flow: 
1. App asks Cache
2. If miss, Cache fetches from DB seamlessly

```
 [ Application ]
           |
           | (1) Write
           v
    +-------------+          +----------+
    |    Cache    | -------> | Database |
    +-------------+   (2)    +----------+
```

- **Pros**: ***High Consistency*** (`Cache = DB`), ***Simple*** App Code
- **Cons**: ***Higher Write Latency*** (Must wait for DB write), ***Rigid*** (Cache structure must match DB).

Use Case: **Critical data where consistency is key (Financial records)**

### Write-Behind or Write-Back

The **"Speed Demon"**

Flow:
1. App writes to Cache
2. Cache immediately Ack's "Success." (Does not wait to communicate write to db)
3. Cache (or worker) ***asynchronously updates DB*** later

```
  [ Application ]
           |
           | (1) Write ((2) Fast "Ack")
           v
    +-------------+           +----------+
    |    Cache    | --(3)-->  | Database |
    +-------------+  Async    +----------+
```

- **Pros**: ***Insane Write Speed***, ***DB Relief*** (Can batch multiple writes into one)
- **Cons**: ***Data Loss Risk*** (If Cache crashes before syncing, data is gone)

Use Case: **Counters (Likes, Views), Analytics, Non-critical high-volume streams**
- Reasoning: It ***prevents database collapse** by aggregating thousands of rapid-fire updates* (like viral likes) *in fast memory and flushing them to the slower database in infrequent batches*, rather than hitting the disk for every single click"

|Strategy|How it works    |Pros |Cons                                              |CAP Fit          |Best For                                     |
|--------|----------------|-----|--------------------------------------------------|-----------------|---------------------------------------------|
|Cache-Aside|App reads DB on miss, then updates Cache.|Resilient (DB is separate).|Stale Data (Gap between DB update & Cache expiry).| **AP (Availability)** |Read-heavy apps (Social Media, News).        |
|Write-Through|App writes to Cache, Cache writes to DB (Sync).|Strong Consistency (Cache == DB).|Slow Writes (Must wait for DB confirmation).      | **CP (Consistency)** |Critical Data (Financials, Inventory).       |
|Write-Behind|App writes to Cache, Cache updates DB later (Async).|Fastest Writes. Reduces DB load.|Data Loss Risk (If Cache crashes before DB sync). |**AP (Availability)** |Counters (Likes, Views), High-Volume Streams.|

## Cache eviction policies for the right cleanup

The cache is ***limited in size***. When it is full, what do we delete?

### Least Recently Used or LRU

**LRU (Least Recently Used) - The Gold Standard**: Remove the item that hasn't been used for the longest time.
- Logic: If you haven't looked at it recently, you probably won't look at it soon.
- Implementation: Doubly Linked List + HashMap (`O(1)` access/removal)
```
      HASH MAP (Fast Lookup)              DOUBLY LINKED LIST (Ordering)
    +-----------------------+            +-----------------------------------+
    | Key A  ----> [Ptr]    |  --------> | [Node A] (MRU - Most Recently Used)|
    | Key B  ----> [Ptr]    |  -----\    |    ^  |                           |
    | Key C  ----> [Ptr]    |  --\   \   |    |  v                           |
    +-----------------------+     \   -> | [Node B]                          |
                                   \     |    ^  |                           |
                                    \    |    |  v                           |
                                     --> | [Node C] (LRU - Least Recently Used)| -> EVICT!
                                         +-----------------------------------+
```
- Use Case: **Almost everything** (Social feeds, product details)

### Least Frequently Used or LFU

Remove the item with the ***fewest total hits***
- Logic: Even if I just read "Contract X" (so it's MRU), if "Contract Y" has been read 10,000 times, keep Y
- Cons: ***Complex to track counts***; new items might get killed before they become popular

### First In First Out or FIFO

The ***queue*** method
- Logic: Oldest object dies
- Cons: ***Bad for "Evergreen" content***. Your homepage might be the "oldest" object but also the most popular

### Time To Live or TTL

TTL (Time To Live) - **The Safety Net**

Not strictly an eviction policy based on space, but ***on time***.
- *Every key* gets an expiry (e.g., `expire_in: 5 minutes`)
- ***Crucial for Cache-Aside***: This is the only way to eventually remove stale data

### Comparison of eviction policies

|Policy|Evicts...       |Best For|Trade-off                                         |
|------|----------------|--------|--------------------------------------------------|
|LRU   |Longest unused  |General (News, Social)|Tracking overhead (List/Map).                     |
|LFU   |Fewest hits     |Frequency (CDN, Images)|Complex; "Stale" popular items stick.             |
|FIFO  |Oldest added    |Age (Logs, Time-series)|Dumb; can evict valid popular data.               |
|TTL   |Expired time    |Freshness (Auth Tokens, OTPs)|Strict; deletes data even if users need it.       |

## Examples for cache strategies or policies

1. "Design Twitter / Facebook Feed"
	- Use: Cache-Aside + Redis Cluster
	- Why: Reading feeds is 99% of traffic. We can tolerate a 5-second delay in seeing a new post (Eventual Consistency)
2. "Design a View Counter"
	- Use: Write-Behind
	- Why: We cannot hit the DB for every view. Buffer 100 views in Redis, then write +100 to SQL once
3. "Design an Authentication System"
	- Use: TTL
	- Why: Store the Session Token in Redis with `TTL=30min`. If the user is *inactive*, it auto-logs them out
4. "Design a Global Static Site"
	- Use: CDN (Geo-Distributed Cache)
	- Why: Serve images from the edge to reduce latency

## How the Thundering Herd problem affects caching

Also known as a **Cache Stampede**

- Scenario: A "Celebrity Key" (e.g., breaking news) is cached. It expires
- The Event: 10,000 users request the key at the exact same millisecond
- The Failure: All 10,000 see a "Miss." All 10,000 hit the Database *simultaneously*. The DB dies

The **Fix**:
1. **Mutex Lock** (Choose this in systemd design interviews): Only allow one request to re-generate the key. The other 9,999 wait
2. **Probabilistic Early Expiration**: If TTL is 60s, start rolling a dice at 50s. If you roll a 1, regenerate the cache before it actually expires
